{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as patches\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.options.display.max_colwidth\npd.options.mode.chained_assignment = None  # default='warn'\n\n\n!pip install openpyxl\n\ndata_2021 = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2021 = data_2021.iloc[0, :].T\ndata_2021 = data_2021.iloc[1:, :]\ndata_2020 = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2020 = data_2020.iloc[0, :].T\ndata_2020 = data_2020.iloc[1:, :]\ndata_2019 = pd.read_csv('../input/kaggle-survey-2019/multiple_choice_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2019 = data_2019.iloc[0, :].T\ndata_2019 = data_2019.iloc[1:, :]\ndata_2018 = pd.read_csv('../input/kaggle-survey-2018/multipleChoiceResponses.csv', low_memory = False, encoding='UTF-8')\nquestions_2018 = data_2018.iloc[0, :].T\ndata_2018 = data_2018.iloc[1:, :]\ndata_2017 = pd.read_csv('../input/kaggle-survey-2017/multipleChoiceResponses.csv', low_memory = False, encoding='ISO-8859-1')\nquestions_2017 = data_2017.iloc[0, :].T\ndata_2017 = data_2017.iloc[1:, :]\n\nnvda_earnings = pd.read_excel('../input/nvda-earnings-2018q1to2021q3/nvda_earnings.xlsx')\ncloud_earnings = pd.read_excel('../input/3-tech-cloud-earnings/cloud.xlsx')\n# 2021 Yes. 2020 Yes. 2019 Yes. About TPUs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-23T05:00:31.103570Z","iopub.execute_input":"2021-11-23T05:00:31.104526Z","iopub.status.idle":"2021-11-23T05:00:48.035711Z","shell.execute_reply.started":"2021-11-23T05:00:31.104475Z","shell.execute_reply":"2021-11-23T05:00:48.034957Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_professionals(data, column):\n    data = data.loc[data[column] != 'Student']\n    data = data.loc[data[column] != 'Currently not employed']\n    data = data.loc[data[column] != 'Not employed']\n    data = data.loc[data[column].notna()]\n    return data\n\npros_2021 = get_professionals(data_2021, 'Q5')\npros_2020 = get_professionals(data_2020, 'Q5')\n\npros_2019 = data_2019[data_2019['Q5'].notna()]\npros_2019 = pros_2019[pros_2019['Q5'] != 'Student']\npros_2019 = pros_2019[pros_2019['Q5'] != 'Not employed']","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:48.037234Z","iopub.execute_input":"2021-11-23T05:00:48.037589Z","iopub.status.idle":"2021-11-23T05:00:48.686682Z","shell.execute_reply.started":"2021-11-23T05:00:48.037560Z","shell.execute_reply":"2021-11-23T05:00:48.685839Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n    \n<div class=\"alert alert-warning\">\n  <strong>Note: </strong> For all charts in this module, I only selected working Professionals.\n</div>\n</div>\n\n<br>\n<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\nNon-professionals were defined as those who answered Job Title as either: \n<ul>\n<li>Student</li>\n<li>Currently not employed</li>\n<li>Who didn't answer the question (NaN)</li>\n</ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<div class=\"alert alert-info\">\n  <strong>Introduction</strong> \n</div>\n\n</div>\n\n<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\nIt has been more than a year and a half since the first lockdown announcement in North America. Cloud penetration was steadily growing before the covid hit our life. The beautiful part is that the cloud is growing even faster in post covid world. I remember on April 30th, Microsoft CEO Satya Nadella said \"We saw 2 years of digital transformation in 2 months\". Pandemic has a big impact not only on our lives but also on the cloud industry.<br>I will go over how the cloud industry evolved pre and post-pandemic eras. 1) What has changed 2) How are the status and 3) what it means for the data science community.\n   \n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n \n**Exhibit 1.0: Cloud, faster and deeper penetration**<Br>\n    <Br>\n    My initial speculation was that Cloud industry to benefit dramatically from the lockdown. Although the proportion of who responded \"none\" is declining steadily, I do not see a dramatic increase in total aggregate Cloud usage in 2020. Another observation is that there a exponential jump in total Cloud usage in 2021. If I account that the world economy suffered in 2020, not declining itself could be interpretated as a another growing year. By Borrowing potential growth concept from Economics, the exponential jump in 2021 is a sort of catchup.\n    <Br>\n         <Br>\n        The Big three; Amazon, Microsoft, Google are dominant forces in cloud industry. There is no dramatic shift in market share. The interesting part here is that Google is just behind the Amazon in kaggle survey. We all acknowledge that google is behind amazon and microsoft at least in the world of cloud industry. Google seems pretty well positionned at least in data science community.<Br><Br>\n<a href=\"https://www.parkmycloud.com/blog/aws-vs-azure-vs-google-cloud-market-share/\">Click If you want to know more about Cloud market share </a>\n    </div>","metadata":{}},{"cell_type":"code","source":"# Cloud usage\n\ncloud_2021 = ['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8',\n 'Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\ndf_2021 = pros_2021[cloud_2021]\ndf_2021\n\ncount_2021 = pd.Series(df_2021[cloud_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_2021 = pd.DataFrame(count_2021)\ndf_count_2021 = df_count_2021.reset_index()\ndf_count_2021.columns = ['Cloud', 'Counts']\n\n# --------------------------------------------\n\ncloud_2020 = ['Q26_A_Part_1','Q26_A_Part_2','Q26_A_Part_3','Q26_A_Part_4','Q26_A_Part_5','Q26_A_Part_6',\n'Q26_A_Part_7','Q26_A_Part_8','Q26_A_Part_9','Q26_A_Part_10','Q26_A_Part_11','Q26_A_OTHER']\ndf_2020 = pros_2020[cloud_2020]\n\ncount_2020 = pd.Series(df_2020[cloud_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_2020 = pd.DataFrame(count_2020)\ndf_count_2020 = df_count_2020.reset_index()\ndf_count_2020.columns = ['Cloud', 'Counts']\n\n# ----------------------------------------------\n\n\ncloud_2019 = ['Q29_Part_1','Q29_Part_2','Q29_Part_3','Q29_Part_4','Q29_Part_5','Q29_Part_6','Q29_Part_7',\n              'Q29_Part_8','Q29_Part_9','Q29_Part_10','Q29_Part_11','Q29_Part_12'] #Q29_OTHER_TEXT\n\ndf_2019 = pros_2019[cloud_2019]\n\ncount_2019 = pd.Series(df_2019[cloud_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_2019 = pd.DataFrame(count_2019)\ndf_count_2019 = df_count_2019.reset_index()\ndf_count_2019.columns = ['Cloud', 'Counts']\ndf_count_2019 = df_count_2019.append({'Cloud':'IBM Cloud / Red Hat', 'Counts': 451}, ignore_index=True).append({'Cloud':'Tencent Cloud', 'Counts': None}, ignore_index=True)\ndf_count_2019 = df_count_2019.drop([4,11]).reset_index(drop = True)\n\n# -------- merge ------\n\n\n\ncloud_df = df_count_2021.merge(df_count_2020, on = 'Cloud').merge(df_count_2019, how = 'left')\ncloud_df['Counts'][4] = 451\ncloud_df = cloud_df.rename(columns = {'Cloud': 'Cloud', 'Counts_x':'2021', 'Counts_y': '2020', 'Counts': '2019'})\ncloud_df_bar = cloud_df.set_index('Cloud').T[::-1]\n\n# --------- 2nd chart -----\n\ncloud_df = cloud_df.T\ncloud_df.columns = cloud_df.iloc[0]\ncloud_df = cloud_df.drop(cloud_df.index[0])\n#cloud_df.index.name = 'Cloud'\ncloud_df = cloud_df.iloc[::-1] # reverse ro\n\nfor_perc = cloud_df\nfor_perc = for_perc.divide(for_perc.sum(axis=1), axis = 0)\n\n# ------------ favorite and 2 years later ----------\n\ncloud_2_year_list = ['Q27_A_Part_1',\n          'Q27_A_Part_2',\n          'Q27_A_Part_3',\n          'Q27_A_Part_4',\n          'Q27_A_Part_5',\n          'Q27_A_Part_6',\n          'Q27_A_Part_7',\n          'Q27_A_Part_8',\n          'Q27_A_Part_9',\n          'Q27_A_Part_10',\n          'Q27_A_Part_11',\n          'Q27_A_OTHER']\n\ncloud_2_year_later = pros_2021[cloud_2_year_list]\n\ncount_cloud_2_year_later = pd.Series(cloud_2_year_later[cloud_2_year_list].squeeze().values.ravel()).value_counts()\ndf_count_cloud_2_year_later = pd.DataFrame(count_cloud_2_year_later).reset_index()\ndf_count_cloud_2_year_later.columns = ['Cloud', '2021']\ndf_count_cloud_2_year_later = df_count_cloud_2_year_later.set_index('Cloud').T\n\n\ncloud_best_exp = pd.Series(pros_2021['Q28'].squeeze().values.ravel()).value_counts()\ndf_cloud_best_exp = pd.DataFrame(cloud_best_exp).reset_index()\ndf_cloud_best_exp.columns = ['Cloud', '2021']\ndf_cloud_best_exp = df_cloud_best_exp.set_index('Cloud').T\n\n\n# ------------- plot ------\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(22,8))\n\n\nplt.subplot(121)   #  subplot 1\n\ncloud_df_bar.plot(kind = 'bar', ax= ax1)\nax1.set(title = \"Faster. Entire industry is growing\",\n       xlabel = \"Year\",\n       ylabel = \"Number of respondents\")\nax1.legend(loc='lower right', bbox_to_anchor=(1.34, 0), prop={'size': 9})\n\nfig.subplots_adjust(wspace=0.4)\n\n\nplt.subplot(122) #  subplot 2\n\nfor_perc.plot(kind='area', \n              stacked=True,\n              ax = ax2,\n             color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f','#7f7f7f')\n             )\nax2.set(title = \"Deeper. respondants with None is going down\",\n       xlabel = \"Year\",\n       ylabel = \"Percentage\")\nax2.legend(loc='lower right', bbox_to_anchor=(1.34, 0), prop={'size': 9})\nax2.annotate('Share of none is decreasing', xy=(1.6, 0.55), xytext=(0.06, 0.6),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:48.688035Z","iopub.execute_input":"2021-11-23T05:00:48.688298Z","iopub.status.idle":"2021-11-23T05:00:49.570688Z","shell.execute_reply.started":"2021-11-23T05:00:48.688270Z","shell.execute_reply":"2021-11-23T05:00:49.570060Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n \n**Exhibit 1.1: Google is well positionned in the world of data science**<Br>\n    <Br>\n        There were decent forward metrics in the survey asking 1) which platform you got the best developer experience and 2) What platform do you hope to become familiar with in 2 years. Based on the previous result, I was expecting #1 AWS, #2 GCP, and #3 Azure. <Br><Br>\n        For the first question, google was just behind amazon as expected. \"They all had a similarly enjoyed developer experience\" was ranked to the 3rd instead of Azure. Given that multi-cloud is pretty common these days, I would classify \"all had a simliar\" option as an alternative to the industry average. That being said, Microsoft's Azure seems quite behind in the data science community. Also, thanks for those honest persons who answered \"None were satisfactory\". <Br><Br>\n        The second question was in line with initial expectation.\n        ","metadata":{}},{"cell_type":"code","source":"cloud_2_year_list = ['Q27_A_Part_1',\n          'Q27_A_Part_2',\n          'Q27_A_Part_3',\n          'Q27_A_Part_4',\n          'Q27_A_Part_5',\n          'Q27_A_Part_6',\n          'Q27_A_Part_7',\n          'Q27_A_Part_8',\n          'Q27_A_Part_9',\n          'Q27_A_Part_10',\n          'Q27_A_Part_11',\n          'Q27_A_OTHER']\n\ncloud_2_year_later = pros_2021[cloud_2_year_list]\n\ncount_cloud_2_year_later = pd.Series(cloud_2_year_later[cloud_2_year_list].squeeze().values.ravel()).value_counts()\ndf_count_cloud_2_year_later = pd.DataFrame(count_cloud_2_year_later).reset_index()\ndf_count_cloud_2_year_later.columns = ['Cloud', '2021']\ndf_count_cloud_2_year_later = df_count_cloud_2_year_later.set_index('Cloud').T\n\n\ncloud_best_exp = pd.Series(pros_2021['Q28'].squeeze().values.ravel()).value_counts()\ndf_cloud_best_exp = pd.DataFrame(cloud_best_exp).reset_index()\ndf_cloud_best_exp.columns = ['Cloud', '2021']\ndf_cloud_best_exp = df_cloud_best_exp.set_index('Cloud').T\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (22,8))\nplt.subplot(121)   #  subplot 1\n\ndf_cloud_best_exp.plot(kind = 'bar', ax = ax1)\nax1.set(title = 'Microsoft Azure is behind the curve',\n       xlabel = \"Year\",\n       ylabel = \"Number of respondents\")\nax2.legend(loc='upper right', bbox_to_anchor=(1.2, 0), prop={'size': 8})\n\nfig.subplots_adjust(wspace=0.4)\n\nplt.subplot(122) #  subplot 2\n\ndf_count_cloud_2_year_later.plot(kind='bar', ax=ax2)\nax2.set(title = 'Cloud platform that respondant is willing to become more familiar in 2 year',\n       xlabel = \"Year\",\n       ylabel = \"Number of respondents\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:49.571772Z","iopub.execute_input":"2021-11-23T05:00:49.572315Z","iopub.status.idle":"2021-11-23T05:00:50.222680Z","shell.execute_reply.started":"2021-11-23T05:00:49.572284Z","shell.execute_reply":"2021-11-23T05:00:50.222116Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n \n**Exhibit 1.2: Preference on google goes beyond amazon**<Br>\n    <Br>\n        There were sister questions asking about cloud computing product usage. The interesting part was the one with \"what kind of cloud platform product is willing to become more familiar in 2 years?\" Google took out AWS in this question and AWS fell back behind Azure. It could be a sign that Amazon will lose steam in 2 years within the data science community. I would be a little cautious to interpret this way because the fallback of AWS could mean that respondents are already too familiar with AWS and they want to expand their experience into the other cloud. Anyhow, the fact is that google received first place by a big margin in this survey. Preference among data science professionals is clear</div>","metadata":{}},{"cell_type":"code","source":"cloud_computing_2019 = ['Q30_Part_1','Q30_Part_2','Q30_Part_3','Q30_Part_4','Q30_Part_5','Q30_Part_6','Q30_Part_7','Q30_Part_8','Q30_Part_9','Q30_Part_10','Q30_Part_11','Q30_OTHER_TEXT']\ndf_computing_2019 = pros_2019[cloud_computing_2019]\ncount_compute_2019 = pd.Series(df_computing_2019[cloud_computing_2019].squeeze().values.ravel()).value_counts()\n\n#-----------------\ndf_count_compute_2019 = pd.DataFrame(count_compute_2019)\ndf_count_compute_2019 = df_count_compute_2019.reset_index()\ndf_count_compute_2019.columns = ['Cloud Compute', 'Counts']\n#------------------\ndf_count_compute_2019['Cloud Compute'] = df_count_compute_2019['Cloud Compute'].str.strip()\ndf_count_compute_2019 = df_count_compute_2019[(df_count_compute_2019['Cloud Compute'] == 'AWS Elastic Compute Cloud (EC2)') |\n                      (df_count_compute_2019['Cloud Compute'] == 'Google Compute Engine (GCE)') |\n                     (df_count_compute_2019['Cloud Compute'] == 'Azure Virtual Machines') |\n                     (df_count_compute_2019['Cloud Compute'] == 'None')\n                                             ]\n#----------------------\ndf_count_compute_2019['Cloud Compute'] = df_count_compute_2019['Cloud Compute'].replace({\n                                                'AWS Elastic Compute Cloud (EC2)': 'Amazon Elastic Compute Cloud (EC2)',\n                                               'Google Compute Engine (GCE)': 'Google Cloud Compute Engine',\n                                                'Azure Virtual Machines': 'Microsoft Azure Virtual Machines',\n                                                'None': 'No / None'\n                                               })\n\n\n\ncloud_computing_2020 = ['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8','Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\ndf_computing_2020 = pros_2020[cloud_computing_2020]\ncount_compute_2020 = pd.Series(df_computing_2020[cloud_computing_2020].squeeze().values.ravel()).value_counts()\n#------------\ndf_count_compute_2020 = pd.DataFrame(count_compute_2020)\ndf_count_compute_2020 = df_count_compute_2020.reset_index()\ndf_count_compute_2020.columns = ['Cloud Compute', 'Counts']\n#--------------\ndf_count_compute_2020['Cloud Compute'] = df_count_compute_2020['Cloud Compute'].str.strip()\ndf_count_compute_2020 = df_count_compute_2020[(df_count_compute_2020['Cloud Compute'] == 'Amazon EC2') |\n                      (df_count_compute_2020['Cloud Compute'] == 'Google Cloud Compute Engine') |\n                     (df_count_compute_2020['Cloud Compute'] == 'Azure Cloud Services') |\n                     (df_count_compute_2020['Cloud Compute'] == 'No / None')\n                     ]\n#--------------\ndf_count_compute_2020['Cloud Compute'] = df_count_compute_2020['Cloud Compute'].replace({\n                                                'Amazon EC2': 'Amazon Elastic Compute Cloud (EC2)',\n                                               'Google Cloud Compute Engine': 'Google Cloud Compute Engine',\n                                                'Azure Cloud Services': 'Microsoft Azure Virtual Machines'\n                                               })\n\n\n\n#----------\ncloud_computing_2021 = ['Q29_A_Part_1','Q29_A_Part_2','Q29_A_Part_3','Q29_A_Part_4','Q29_A_OTHER']\ndf_computing_2021 = pros_2021[cloud_computing_2021]\ncount_compute_2021 = pd.Series(df_computing_2021[cloud_computing_2021].squeeze().values.ravel()).value_counts()\n# ----------\ndf_count_compute_2021 = pd.DataFrame(count_compute_2021)\ndf_count_compute_2021 = df_count_compute_2021.reset_index()\ndf_count_compute_2021.columns = ['Cloud Compute', 'Counts']\n#------------\ndf_count_compute_2021['Cloud Compute'] = df_count_compute_2021['Cloud Compute'].str.strip()\ndf_count_compute_2021 = df_count_compute_2021[(df_count_compute_2021['Cloud Compute'] != 'Other')]\n\ncloud_compute_df = df_count_2021.merge(df_count_2020, on = 'Cloud').merge(df_count_2019, how = 'left')\ncloud_compute_df = df_count_compute_2021.merge(df_count_compute_2020, on = 'Cloud Compute').merge(df_count_compute_2019, how = 'left')\ncloud_compute_df = cloud_compute_df.rename(columns = {'Cloud': 'Cloud', 'Counts_x':'2021', 'Counts_y': '2020', 'Counts': '2019'})\n\ncloud_compute_df = cloud_compute_df.T\ncloud_compute_df.columns = cloud_compute_df.iloc[0]\ncloud_compute_df = cloud_compute_df.drop(cloud_compute_df.index[0])\n#cloud_df.index.name = 'Cloud'\ncloud_compute_df = cloud_compute_df.iloc[::-1] # reverse ro\n\n# ---- percentage ----\n\nfor_perc = cloud_compute_df\nfor_perc = for_perc.divide(for_perc.sum(axis=1), axis = 0)\n\n# ------- in 2 years --------\n\n\ncloud_computing_in_2 = ['Q29_B_Part_1','Q29_B_Part_2','Q29_B_Part_3','Q29_B_Part_4','Q29_B_OTHER']\n\ncount_cloud_computing_in_2 = pd.Series(pros_2021[cloud_computing_in_2].squeeze().values.ravel()).value_counts()\n\ndf_count_cloud_computing_in_2 = pd.DataFrame(count_cloud_computing_in_2)\ndf_count_cloud_computing_in_2 = df_count_cloud_computing_in_2.reset_index()\ndf_count_cloud_computing_in_2.columns = ['Cloud Compute', '2021']\ndf_count_cloud_computing_in_2 = df_count_cloud_computing_in_2.set_index('Cloud Compute').T\n\n\n# --- plot ---\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (22,8))\nplt.subplot(131)   #  subplot 1\n\ncloud_compute_df.plot(kind = 'bar', ax= ax1)\nax1.set(title = \"Cloud platform product usage in regular basis in kaggle survey\",\n        xlabel = \"Year\",\n       ylabel = \"the number of respondents\")\nax1.legend(loc='lower right', bbox_to_anchor=(1.35, 0), prop={'size': 9})\n\n\nfig.subplots_adjust(wspace=0.4)\n\nplt.subplot(132) #  subplot 2\n\nfor_perc.plot(kind='area', \n              stacked=True,\n              ax = ax2,\n             color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',)\n             )\nax2.set(title = \"Cloud platform product usage by share\",\n        xlabel = \"Year\",\n       ylabel = \"the number of respondents\")\nax2.legend(loc='lower right', bbox_to_anchor=(1.36, 0), prop={'size': 9})\nax2.annotate('Share of none is decreasing', xy=(1.6, 0.62), xytext=(0.06, 0.7),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )\n\nplt.subplot(133)\n\ndf_count_cloud_computing_in_2.plot(kind='bar', ax=ax3, color=['#ff7f0e','#d62728','#1f77b4','#2ca02c','#7f7f7f'])\nax3.set(title = \"respondent willing to become more familiar in 2 year\",\n       xlabel = \"Year\",\n       ylabel = \"the number of respondents\")\nax3.legend(loc='lower right', bbox_to_anchor=(1.36, 0), prop={'size': 9})\nax3.text(0.01, 2500, 'AWS is pushed back to the third place',\n        verticalalignment='bottom',\n        fontsize=12)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:50.224417Z","iopub.execute_input":"2021-11-23T05:00:50.224729Z","iopub.status.idle":"2021-11-23T05:00:51.004864Z","shell.execute_reply.started":"2021-11-23T05:00:50.224702Z","shell.execute_reply":"2021-11-23T05:00:51.004163Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let's examine first how the big data product market is. The big data market could be divided into two big categories; open-source and commercial. I was expecting commercial database products to exceed or to outpace open-source products because database products have been existed for quite a long time compared to any other cloud features. The result shows the complete opposite. ","metadata":{}},{"cell_type":"code","source":"big_data_2021 = ['Q32_A_Part_1','Q32_A_Part_2',\n                 'Q32_A_Part_3','Q32_A_Part_4',\n                 'Q32_A_Part_5','Q32_A_Part_6',\n                 'Q32_A_Part_7','Q32_A_Part_8',\n                 'Q32_A_Part_9','Q32_A_Part_10',\n                 'Q32_A_Part_11','Q32_A_Part_12',\n                 'Q32_A_Part_13','Q32_A_Part_14',\n                 'Q32_A_Part_15','Q32_A_Part_16',\n                 'Q32_A_Part_17','Q32_A_Part_18',\n                 'Q32_A_Part_19','Q32_A_Part_20','Q32_A_OTHER']\n\n\n\ndf_bigdata_2021 = pros_2021[big_data_2021]\ncount_bigdata_2021 = pd.Series(df_bigdata_2021[big_data_2021].squeeze().values.ravel()).value_counts()\n#count_bigdata_2021\n\ndf_count_bigdata_2021 = pd.DataFrame(count_bigdata_2021)\ndf_count_bigdata_2021 = df_count_bigdata_2021.reset_index()\ndf_count_bigdata_2021.columns = ['big data', 'Counts']\n\n# --------------------\n\nbig_data_2020 = ['Q29_A_Part_1','Q29_A_Part_2',\n                 'Q29_A_Part_3','Q29_A_Part_4',\n                 'Q29_A_Part_5','Q29_A_Part_6',\n                 'Q29_A_Part_7','Q29_A_Part_8',\n                 'Q29_A_Part_9','Q29_A_Part_10',\n                 'Q29_A_Part_11','Q29_A_Part_12',\n                 'Q29_A_Part_13','Q29_A_Part_14',\n                 'Q29_A_Part_15','Q29_A_Part_16',\n                 'Q29_A_Part_17','Q29_A_OTHER']\n\ndf_bigdata_2020 = pros_2020[big_data_2020]\ncount_bigdata_2020 = pd.Series(df_bigdata_2020[big_data_2020].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_bigdata_2020 = pd.DataFrame(count_bigdata_2020)\ndf_count_bigdata_2020 = df_count_bigdata_2020.reset_index()\ndf_count_bigdata_2020.columns = ['big data', 'Counts']\n\n# ----------------------\n\nbig_data_2019 = ['Q34_Part_1','Q34_Part_2',\n                 'Q34_Part_3','Q34_Part_4',\n                 'Q34_Part_5','Q34_Part_6',\n                 'Q34_Part_7','Q34_Part_8',\n                 'Q34_Part_9','Q34_Part_10',\n                 'Q34_Part_11','Q34_Part_12',\n                 'Q34_OTHER_TEXT']\n\ndf_bigdata_2019 = pros_2019[big_data_2019]\ncount_bigdata_2019 = pd.Series(df_bigdata_2019[big_data_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_bigdata_2019 = pd.DataFrame(count_bigdata_2019)\ndf_count_bigdata_2019 = df_count_bigdata_2019.reset_index()\ndf_count_bigdata_2019.columns = ['big data', 'Counts']\ndf_count_bigdata_2019 = df_count_bigdata_2019[(df_count_bigdata_2019['big data'] == 'MySQL') |\n                                              (df_count_bigdata_2019['big data'] == 'PostgresSQL') |\n                                              (df_count_bigdata_2019['big data'] == 'Microsoft SQL Server') |\n                                              (df_count_bigdata_2019['big data'] == 'SQLite') |\n                                              (df_count_bigdata_2019['big data'] == 'Oracle Database') |\n                                              (df_count_bigdata_2019['big data'] == 'AWS Relational Database Service') |\n                                              (df_count_bigdata_2019['big data'] == 'Microsoft Access') |\n                                              (df_count_bigdata_2019['big data'] == 'Google Cloud SQL') |\n                                              (df_count_bigdata_2019['big data'] == 'Azure SQL Database') |\n                                              (df_count_bigdata_2019['big data'] == 'AWS DynamoDB')\n                                             ]\n\ndef open_source(x):\n    if x == 'MySQL':\n        return 1\n    elif x == 'PostgreSQL':\n        return 1\n    elif x == 'MongoDB':\n        return 1\n    elif x == 'SQLite':\n        return 1\n    elif x == 'PostgresSQL':\n        return 1\n    else:\n        return 0\n    \ndef aws(x):\n    if 'Amazon' in x:\n        return 1\n    elif 'AWS' in x:\n        return 1\n    else:\n        return 0\n    \ndef gcp(x):\n    if 'Google' in x:\n        return 1\n    else:\n        return 0\n    \ndef azure(x):\n    if 'Microsoft' in x:\n        return 1\n    elif 'Azure' in x:\n        return 1\n    else: return 0\n\ndf_count_bigdata_2021['big data'] = df_count_bigdata_2021['big data'].str.strip()\ndf_count_bigdata_2021['open source'] = df_count_bigdata_2021['big data'].apply(open_source)\ndf_count_bigdata_2021['aws'] = df_count_bigdata_2021['big data'].apply(aws)\ndf_count_bigdata_2021['gcp'] = df_count_bigdata_2021['big data'].apply(gcp)\ndf_count_bigdata_2021['azure'] = df_count_bigdata_2021['big data'].apply(azure)\n\nopen_source_2021 = df_count_bigdata_2021[df_count_bigdata_2021['open source'] == 1]\nopen_source_2021 = open_source_2021[['big data', 'Counts']]\n\naws_2021 = df_count_bigdata_2021[df_count_bigdata_2021['aws'] == 1]\ngcp_2021 = df_count_bigdata_2021[df_count_bigdata_2021['gcp'] == 1]\nazure_2021 = df_count_bigdata_2021[df_count_bigdata_2021['azure'] == 1]\nothers_2021 = df_count_bigdata_2021[(df_count_bigdata_2021['aws'] != 1) & \n                                    (df_count_bigdata_2021['gcp'] != 1) & \n                                    (df_count_bigdata_2021['azure'] != 1) &\n                                    (df_count_bigdata_2021['open source'] != 1)]\n\naws_2021['big data'].iloc[0] = 'aws'\ngcp_2021['big data'].iloc[0] = 'gcp'\nazure_2021['big data'].iloc[0] = 'azure'\n\ncommercial_2021 = aws_2021.iloc[:1].append([gcp_2021.iloc[:1], azure_2021.iloc[:1], others_2021])\nto_drop = ['None', 'Other']\ncommercial_2021 = commercial_2021[~commercial_2021['big data'].isin(to_drop)]\ncommercial_2021 = commercial_2021[['big data', 'Counts']]\n\nopen_source_2021 = open_source_2021.merge(commercial_2021, on = 'big data', how = 'outer')\nopen_source_2021 = open_source_2021.rename(columns = {'Counts_x' : 'open source_2021', 'Counts_y' : 'commercial_2021'})\n\nopen_source_2021 = open_source_2021.set_index('big data').T\n\n#fig, ax1 = plt.subplots(figsize = (16,8))\n#open_source_2021.plot(kind = 'bar', stacked = True, ax = ax1)\n\n\n# -------------\n\ndf_count_bigdata_2020['big data'] = df_count_bigdata_2020['big data'].str.strip()\ndf_count_bigdata_2020['open source'] = df_count_bigdata_2020['big data'].apply(open_source)\ndf_count_bigdata_2020['aws'] = df_count_bigdata_2020['big data'].apply(aws)\ndf_count_bigdata_2020['gcp'] = df_count_bigdata_2020['big data'].apply(gcp)\ndf_count_bigdata_2020['azure'] = df_count_bigdata_2020['big data'].apply(azure)\n\nopen_source_2020 = df_count_bigdata_2020[df_count_bigdata_2020['open source'] == 1]\nopen_source_2020 = open_source_2020[['big data', 'Counts']]\n\naws_2020 = df_count_bigdata_2020[df_count_bigdata_2020['aws'] == 1]\ngcp_2020 = df_count_bigdata_2020[df_count_bigdata_2020['gcp'] == 1]\nazure_2020 = df_count_bigdata_2020[df_count_bigdata_2020['azure'] == 1]\nothers_2020 = df_count_bigdata_2020[(df_count_bigdata_2020['aws'] != 1) & \n                                    (df_count_bigdata_2020['gcp'] != 1) & \n                                    (df_count_bigdata_2020['azure'] != 1) &\n                                    (df_count_bigdata_2020['open source'] != 1)]\n\naws_2020['big data'].iloc[0] = 'aws'\ngcp_2020['big data'].iloc[0] = 'gcp'\nazure_2020['big data'].iloc[0] = 'azure'\n\ncommercial_2020 = aws_2020.iloc[:1].append([gcp_2020.iloc[:1], azure_2020.iloc[:1], others_2020])\nto_drop = ['None', 'Other']\ncommercial_2020 = commercial_2020[~commercial_2020['big data'].isin(to_drop)]\ncommercial_2020 = commercial_2020[['big data', 'Counts']]\n\nopen_source_2020 = open_source_2020.merge(commercial_2020, on = 'big data', how = 'outer')\nopen_source_2020 = open_source_2020.rename(columns = {'Counts_x' : 'open source_2020', 'Counts_y' : 'commercial_2020'})\n\nopen_source_2020 = open_source_2020.set_index('big data').T\n\n# -------------\n\ndf_count_bigdata_2019['big data'] = df_count_bigdata_2019['big data'].str.strip()\ndf_count_bigdata_2019['open source'] = df_count_bigdata_2019['big data'].apply(open_source)\ndf_count_bigdata_2019['aws'] = df_count_bigdata_2019['big data'].apply(aws)\ndf_count_bigdata_2019['gcp'] = df_count_bigdata_2019['big data'].apply(gcp)\ndf_count_bigdata_2019['azure'] = df_count_bigdata_2019['big data'].apply(azure)\n\nopen_source_2019 = df_count_bigdata_2019[df_count_bigdata_2019['open source'] == 1]\nopen_source_2019 = open_source_2019[['big data', 'Counts']]\n\naws_2019 = df_count_bigdata_2019[df_count_bigdata_2019['aws'] == 1]\ngcp_2019 = df_count_bigdata_2019[df_count_bigdata_2019['gcp'] == 1]\nazure_2019 = df_count_bigdata_2019[df_count_bigdata_2019['azure'] == 1]\nothers_2019 = df_count_bigdata_2019[(df_count_bigdata_2019['aws'] != 1) & \n                                    (df_count_bigdata_2019['gcp'] != 1) & \n                                    (df_count_bigdata_2019['azure'] != 1) &\n                                    (df_count_bigdata_2019['open source'] != 1)]\n\naws_2019['big data'].iloc[0] = 'aws'\ngcp_2019['big data'].iloc[0] = 'gcp'\nazure_2019['big data'].iloc[0] = 'azure'\n\ncommercial_2019 = aws_2019.iloc[:1].append([gcp_2019.iloc[:1], azure_2019.iloc[:1], others_2019])\nto_drop = ['None', 'Other']\ncommercial_2019 = commercial_2019[~commercial_2019['big data'].isin(to_drop)]\ncommercial_2019 = commercial_2019[['big data', 'Counts']]\n\nopen_source_2019 = open_source_2019.merge(commercial_2019, on = 'big data', how = 'outer')\nopen_source_2019 = open_source_2019.rename(columns = {'Counts_x' : 'open source_2019', 'Counts_y' : 'commercial_2019'})\n\nopen_source_2019 = open_source_2019.set_index('big data').T\n\nbig_data_usage = pd.concat([open_source_2021, open_source_2020, open_source_2019])\nbig_data_usage['PostgresSQL'] = big_data_usage['PostgresSQL'].fillna(big_data_usage['PostgreSQL'])\nbig_data_usage = big_data_usage.drop(['PostgreSQL'], axis = 1)\n\n\n# --- plot ----\n\nbig_data_usage[\"total\"] = big_data_usage.sum(axis=1)\nexample = big_data_usage['total']\nexample_2021 = example[:2]\nexample_2020 = example[2:4]\nexample_2019 = example[4:6]\n\nk_2021 = pd.DataFrame(example_2021)\nk_2021 = k_2021.rename(index = ({'open source_2021': 'open source', 'commercial_2021': 'commercial'}), columns=({'total': '2021'}))\n\n\nk_2020 = pd.DataFrame(example_2020)\nk_2020 = k_2020.rename(index = ({'open source_2020': 'open source', 'commercial_2020': 'commercial'}), columns=({'total': '2020'}))\n\n\n\nk_2019 = pd.DataFrame(example_2019)\nk_2019 = k_2019.rename(index = ({'open source_2019': 'open source', 'commercial_2019': 'commercial'}), columns=({'total': '2019'}))\n\nk_2021['2020'] = k_2020\nk_2021['2019'] = k_2019\nk_2021 = k_2021.iloc[:, ::-1]\nk_2021 = k_2021.T\n\nfig, ax = plt.subplots(figsize = (18,10))\nk_2021.plot(kind='bar', stacked = True, ax=ax)\nax.set(title = \"Open source and commercial big data product usage\",\n      xlabel = \"Year\",\n      ylabel = \"the number of respondents\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:51.006293Z","iopub.execute_input":"2021-11-23T05:00:51.006896Z","iopub.status.idle":"2021-11-23T05:00:51.428427Z","shell.execute_reply.started":"2021-11-23T05:00:51.006856Z","shell.execute_reply":"2021-11-23T05:00:51.427531Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"MongoDB, the only non-relational database in the survey was added back in the 2020 survey and takes roughly 20% of the pie. MySQL remains dominant. \nIn the commercial database, Azure and oracle lead the market. If I account Oracle is the owner of MySQL, the two accounts more than 50% of the pie. Snowflake and IBM Db2 are newly added in 2020. The interesting point is the rise of Snowflake in 2021. ","metadata":{}},{"cell_type":"code","source":"big_data_usage.drop(['total'], axis = 1, inplace = True)\nbig_data_usage = big_data_usage.T\n\nbig_data_usage_open_source = big_data_usage[['open source_2019', 'open source_2020', 'open source_2021']]\nbig_data_usage_open_source.dropna(axis = 0, how = 'all', inplace = True)\n\nbig_data_usage_commercial = big_data_usage[['commercial_2019', 'commercial_2020', 'commercial_2021']]\nbig_data_usage_commercial.dropna(axis = 0, how = 'all', inplace = True)\n\nfig, axes = plt.subplots(1,3, figsize=(16,8))\nfor ax, col in zip(axes, big_data_usage_open_source.columns):\n    big_data_usage_open_source[col].plot(kind='pie', legend=False, ax=ax, autopct='%0.2f', title=col)\n    ax.set(ylabel='', aspect='equal')\n\n#axes[0].legend(bbox_to_anchor=(0, 0.5))\n\n\nfig, axes = plt.subplots(1,3, figsize=(16,8))\nfor ax, col in zip(axes, big_data_usage_commercial.columns):\n    big_data_usage_commercial[col].plot(kind='pie', legend=False, ax=ax, autopct='%0.2f', title=col)\n    ax.set(ylabel='', aspect='equal')\n\n#axes[0].legend(bbox_to_anchor=(0, 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:51.429645Z","iopub.execute_input":"2021-11-23T05:00:51.429881Z","iopub.status.idle":"2021-11-23T05:00:52.269083Z","shell.execute_reply.started":"2021-11-23T05:00:51.429853Z","shell.execute_reply":"2021-11-23T05:00:52.268052Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This is the micro picture. In the open-source section, respondents picked MongoDB as the second most often used. In the Commercial section,  Notable strength in the Google family when it comes to the hope to get familiar in 2 years.","metadata":{}},{"cell_type":"code","source":"big_data_2021 = ['Q32_A_Part_1','Q32_A_Part_2',\n                 'Q32_A_Part_3','Q32_A_Part_4',\n                 'Q32_A_Part_5','Q32_A_Part_6',\n                 'Q32_A_Part_7','Q32_A_Part_8',\n                 'Q32_A_Part_9','Q32_A_Part_10',\n                 'Q32_A_Part_11','Q32_A_Part_12',\n                 'Q32_A_Part_13','Q32_A_Part_14',\n                 'Q32_A_Part_15','Q32_A_Part_16',\n                 'Q32_A_Part_17','Q32_A_Part_18',\n                 'Q32_A_Part_19','Q32_A_Part_20','Q32_A_OTHER']\n\n\n\ndf_bigdata_2021 = pros_2021[big_data_2021]\ncount_bigdata_2021 = pd.Series(df_bigdata_2021[big_data_2021].squeeze().values.ravel()).value_counts()\n#count_bigdata_2021\n\ndf_count_bigdata_2021 = pd.DataFrame(count_bigdata_2021)\ndf_count_bigdata_2021 = df_count_bigdata_2021.reset_index()\ndf_count_bigdata_2021.columns = ['big data', 'Counts']\ndf_count_bigdata_2021 = df_count_bigdata_2021.set_index('big data').T\ndf_count_bigdata_2021.columns = df_count_bigdata_2021.columns.str.strip()\n\ndf_count_bigdata_2021_open_source = pd.DataFrame()\ndf_count_bigdata_2021_commercial = pd.DataFrame()\n\ndf_count_bigdata_2021_open_source = df_count_bigdata_2021[['MySQL', 'PostgreSQL', 'MongoDB', 'SQLite']]\ndf_count_bigdata_2021_commercial = df_count_bigdata_2021[['Microsoft SQL Server', \n                                                          'Oracle Database',\n                                                         'Google Cloud BigQuery',\n                                                         'Microsoft Azure SQL Database',\n                                                         'Amazon RDS',\n                                                         'Google Cloud SQL',\n                                                         'Amazon Redshift',\n                                                         'Snowflake',\n                                                         'Amazon DynamoDB',\n                                                         'Microsoft Azure Cosmos DB',\n                                                         'Google Cloud Firestore',\n                                                         'IBM Db2',\n                                                         'Google Cloud BigTable',\n                                                         'Amazon Aurora',\n                                                         'Google Cloud Spanner']]\n\n\nmost_often_dbs = pd.DataFrame(pros_2021['Q33'].value_counts())\nmost_often_dbs = most_often_dbs.T\nmost_often_dbs.columns = most_often_dbs.columns.str.strip()\nmost_often_dbs = most_often_dbs[['MySQL', 'PostgreSQL', 'MongoDB', 'SQLite']]\nmost_often_dbs = most_often_dbs.T\nmost_often_dbs = most_often_dbs.reset_index()\nmost_often_dbs = most_often_dbs.rename(columns = {'index':'big data'})\n\n# ---------------------\n\nbig_data_2_year = [\n    'Q32_B_Part_1',\n    'Q32_B_Part_2',\n    'Q32_B_Part_3',\n    'Q32_B_Part_4',\n    'Q32_B_Part_5',\n    'Q32_B_Part_6',\n    'Q32_B_Part_7',\n    'Q32_B_Part_8',\n    'Q32_B_Part_9',\n    'Q32_B_Part_10',\n    'Q32_B_Part_11',\n    'Q32_B_Part_12',\n    'Q32_B_Part_13',\n    'Q32_B_Part_14',\n    'Q32_B_Part_15',\n    'Q32_B_Part_16',\n    'Q32_B_Part_17',\n    'Q32_B_Part_18',\n    'Q32_B_Part_19',\n    'Q32_B_Part_20',\n    'Q32_B_OTHER'\n]\n\n\n\n\ndf_bigdata_2021_2 = pros_2021[big_data_2_year]\ncount_bigdata_2021_2 = pd.Series(df_bigdata_2021_2[big_data_2_year].squeeze().values.ravel()).value_counts()\n#count_bigdata_2021\n\ndf_count_bigdata_2021_2 = pd.DataFrame(count_bigdata_2021_2)\ndf_count_bigdata_2021_2 = df_count_bigdata_2021_2.reset_index()\ndf_count_bigdata_2021_2.columns = ['big data', 'Counts']\n\ndf_count_bigdata_2021_2 = df_count_bigdata_2021_2.set_index('big data').T\ndf_count_bigdata_2021_2\n\n# ---------------------------\n\ndf_count_bigdata_2021_2.columns = df_count_bigdata_2021_2.columns.str.strip()\ndf_count_bigdata_2021_2_open = df_count_bigdata_2021_2[['MySQL', 'PostgreSQL', 'MongoDB', 'SQLite']]\ndf_count_bigdata_2021_2_open = df_count_bigdata_2021_2_open.T\ndf_count_bigdata_2021_2_open = df_count_bigdata_2021_2_open.reset_index()\n\ndf_count_bigdata_2021_open_source = df_count_bigdata_2021_open_source.T\ndf_count_bigdata_2021_open_source = df_count_bigdata_2021_open_source.reset_index()\n\nmerged_micro_db = df_count_bigdata_2021_open_source.merge(df_count_bigdata_2021_2_open, on ='big data').merge(most_often_dbs, on = 'big data')\nmerged_micro_db = merged_micro_db.rename(columns = {'Counts_x': 'regularly use',\n                                                   'Counts_y': 'Most often use',\n                                                   'Q33': 'Hope to get familiar in 2 years'})\nmerged_micro_db_open = merged_micro_db.set_index('big data').T\n\nmost_often_dbs = pd.DataFrame(pros_2021['Q33'].value_counts())\nmost_often_dbs = most_often_dbs.T\nmost_often_dbs.columns = most_often_dbs.columns.str.strip()\nmost_often_dbs = most_often_dbs[['Microsoft SQL Server', \n                                                          'Oracle Database',\n                                                         'Google Cloud BigQuery',\n                                                         'Microsoft Azure SQL Database',\n                                                         'Amazon RDS',\n                                                         'Google Cloud SQL',\n                                                         'Amazon Redshift',\n                                                         'Snowflake',\n                                                         'Amazon DynamoDB',\n                                                         'Microsoft Azure Cosmos DB',\n                                                         'Google Cloud Firestore',\n                                                         'IBM Db2',\n                                                         'Google Cloud BigTable',\n                                                         'Amazon Aurora',\n                                                         'Google Cloud Spanner']]\nmost_often_dbs = most_often_dbs.T\nmost_often_dbs = most_often_dbs.reset_index()\nmost_often_dbs = most_often_dbs.rename(columns = {'index':'big data'})\n\n\n#df_count_bigdata_2021_2 = df_count_bigdata_2021_2.set_index('big data').T\n\n#fig, ax = plt.subplots(figsize = (22,8))\n#df_count_bigdata_2021_2.plot(kind='bar', ax =ax)\n\ndf_count_bigdata_2021_2.columns = df_count_bigdata_2021_2.columns.str.strip()\ndf_count_bigdata_2021_2_commercial = df_count_bigdata_2021_2[['Microsoft SQL Server', \n                                                          'Oracle Database',\n                                                         'Google Cloud BigQuery',\n                                                         'Microsoft Azure SQL Database',\n                                                         'Amazon RDS',\n                                                         'Google Cloud SQL',\n                                                         'Amazon Redshift',\n                                                         'Snowflake',\n                                                         'Amazon DynamoDB',\n                                                         'Microsoft Azure Cosmos DB',\n                                                         'Google Cloud Firestore',\n                                                         'IBM Db2',\n                                                         'Google Cloud BigTable',\n                                                         'Amazon Aurora',\n                                                         'Google Cloud Spanner']]\n\ndf_count_bigdata_2021_2_commercial = df_count_bigdata_2021_2_commercial.T\ndf_count_bigdata_2021_2_commercial = df_count_bigdata_2021_2_commercial.reset_index()\n\ndf_count_bigdata_2021_commercial = df_count_bigdata_2021_commercial.T\ndf_count_bigdata_2021_commercial = df_count_bigdata_2021_commercial.reset_index()\n\nmerged_micro_db = df_count_bigdata_2021_commercial.merge(most_often_dbs, on = 'big data').merge(df_count_bigdata_2021_2_commercial, on ='big data')\nmerged_micro_db = merged_micro_db.rename(columns = {'Counts_x': 'regularly use',\n                                                   'Counts_y': 'Hope to get familiar in 2 years',\n                                                   'Q33': 'Most often use'})\nmerged_micro_db_commercial = merged_micro_db.set_index('big data').T\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (22,8))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 3]) \nax1 = plt.subplot(gs[0])\n\nmerged_micro_db_open.plot(kind='bar', ax= ax1)\n\nax1.set(title = \"Open source big data product\",\n      xlabel = \"Survey questions\",\n      ylabel = \"Counts\")\n\nax1.annotate('MongoDB > PostgreSQL', xy=(1, 1800), xytext=(0.06, 2000),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )\n\nax2 = plt.subplot(gs[1])\n\nmerged_micro_db_commercial.plot(kind='bar', ax= ax2)\n\nax2.set(title = \"Commercial big data product\",\n      xlabel = \"Survey questions\",\n      ylabel = \"Counts\")\n\nax2.annotate('Notable strength of Google family', xy=(2, 1500), xytext=(1.4, 1800),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:52.270666Z","iopub.execute_input":"2021-11-23T05:00:52.270963Z","iopub.status.idle":"2021-11-23T05:00:53.208778Z","shell.execute_reply.started":"2021-11-23T05:00:52.270924Z","shell.execute_reply":"2021-11-23T05:00:53.207814Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Before jumping into the business intelligence market status, let's see data visualization library usage in the Kaggle survey. Matplotlib and Seaborn are the most widely used python visualization libraries. The followings are plotly and ggplot. ggplot is used for r community, and plotly is known as interactive friendly library for python community.\n\nThe average library usage growth in 2020 and 2021 is 6% and 38%. Libraries that showed sequential growth that topped the average are Matplotlib, seaborn, plotly, geoplolib and Altair. Since the covid, interactive covid dashboard along with geographic feature.\n\nMatplotlib and seaborn are the two gold-standard python data science libraries for the data viz. The rise in plobtly, geoplotlib and altair seem to be linked more in the new fresh demand in interactive geographic visualization.","metadata":{}},{"cell_type":"code","source":"viz_bi_2020 =['Q14_Part_1','Q14_Part_2',\n              'Q14_Part_3','Q14_Part_4',\n              'Q14_Part_5','Q14_Part_6',\n              'Q14_Part_7','Q14_Part_8',\n              'Q14_Part_9','Q14_Part_10',\n              'Q14_Part_11','Q14_OTHER',\n              'Q31_A_Part_1','Q31_A_Part_2',\n              'Q31_A_Part_3','Q31_A_Part_4',\n              'Q31_A_Part_5','Q31_A_Part_6',\n              'Q31_A_Part_7','Q31_A_Part_8',\n              'Q31_A_Part_9','Q31_A_Part_10',\n              'Q31_A_Part_11','Q31_A_Part_12',\n              'Q31_A_Part_13','Q31_A_Part_14','Q31_A_OTHER']\n\nlib_BI_2020 = pros_2020[viz_bi_2020].rename(columns ={\n    'Q14_Part_1': 'Matplotlib',\n    'Q14_Part_2': 'Seaborn',\n    'Q14_Part_3': 'Plotly',\n    'Q14_Part_4': 'Ggplot',\n    'Q14_Part_5': 'Shiny',\n    'Q14_Part_6': 'D3 js',\n    'Q14_Part_7': 'Altair',\n    'Q14_Part_8': 'Bokeh',\n    'Q14_Part_9': 'Geoplotlib',\n    'Q14_Part_10': 'Leaflet',\n    'Q14_Part_11': 'None',\n    'Q14_OTHER': 'Other',\n    \n    'Q31_A_Part_1': 'Amazon QuickSight',\n    'Q31_A_Part_2': 'MS Power BI',\n    'Q31_A_Part_3': 'Google Data Studio',\n    'Q31_A_Part_4': 'Looker',\n    'Q31_A_Part_5': 'Tableau',\n    'Q31_A_Part_6': 'Salesforce',\n    'Q31_A_Part_7': 'Einstein Analytics',\n    'Q31_A_Part_8': 'Qlik',\n    'Q31_A_Part_9': 'Domo',\n    'Q31_A_Part_10': 'TIBCO',\n    'Q31_A_Part_11': 'Alteryx',\n    'Q31_A_Part_12': 'Sisense',\n    'Q31_A_Part_13': 'SAP',\n    'Q31_A_Part_14': 'None',\n    'Q31_A_OTHER': 'Other'\n})\n\nlib_2020 = ['Matplotlib',\n    'Seaborn',\n    'Plotly',\n    'Ggplot']\n\nBI_2020 = ['Amazon QuickSight',\n    'MS Power BI',\n    'Google Data Studio',\n    'Looker',\n    'Tableau',\n    'Salesforce',\n    'Einstein Analytics',\n    'Qlik',\n    'Domo',\n    'TIBCO',\n    'Alteryx',\n    'Sisense',\n    'SAP'\n    ]\n# ----- library growth -----\n\ndata_viz_li_2019 = ['Q20_Part_1',\n                    'Q20_Part_2',\n                    'Q20_Part_3',\n                    'Q20_Part_4',\n                    'Q20_Part_5',\n                    'Q20_Part_6',\n                    'Q20_Part_7',\n                    'Q20_Part_8',\n                    'Q20_Part_9',\n                    'Q20_Part_10',\n                    'Q20_Part_11',\n                    'Q20_Part_12']\n\ndf_data_viz_li_2019 = pros_2019[data_viz_li_2019]\n\n\ncount_data_viz_li_2019 = pd.Series(df_data_viz_li_2019[data_viz_li_2019].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2019 = pd.DataFrame(count_data_viz_li_2019)\ndf_count_data_viz_li_2019 = df_count_data_viz_li_2019.reset_index()\ndf_count_data_viz_li_2019.columns = ['lib', 'Counts']\ndf_count_data_viz_li_2019\n\n\ndata_viz_li_2020 =['Q14_Part_1','Q14_Part_2',\n              'Q14_Part_3','Q14_Part_4',\n              'Q14_Part_5','Q14_Part_6',\n              'Q14_Part_7','Q14_Part_8',\n              'Q14_Part_9','Q14_Part_10',\n              'Q14_Part_11','Q14_OTHER']\n\ndf_data_viz_li_2020 = pros_2020[data_viz_li_2020]\n\n\ncount_data_viz_li_2020 = pd.Series(df_data_viz_li_2020[data_viz_li_2020].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2020 = pd.DataFrame(count_data_viz_li_2020)\ndf_count_data_viz_li_2020 = df_count_data_viz_li_2020.reset_index()\ndf_count_data_viz_li_2020.columns = ['lib', 'Counts']\ndf_count_data_viz_li_2020\n\nviz_bi_2021 = ['Q14_Part_1','Q14_Part_2',\n                         'Q14_Part_3','Q14_Part_4',\n                         'Q14_Part_5','Q14_Part_6',\n                         'Q14_Part_7','Q14_Part_8',\n                         'Q14_Part_9','Q14_Part_10',\n                         'Q14_Part_11','Q14_OTHER']\n\ndf_data_viz_li_2021 = pros_2021[viz_bi_2021]\n\n\ncount_data_viz_li_2021 = pd.Series(df_data_viz_li_2021[viz_bi_2021].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2021 = pd.DataFrame(count_data_viz_li_2021)\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.reset_index()\ndf_count_data_viz_li_2021.columns = ['lib', 'Counts']\n\n# ---------------\n\n#print(df_count_data_viz_li_2021)\n#print(df_count_data_viz_li_2020)\n#print(df_count_data_viz_li_2019)\n\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.merge(df_count_data_viz_li_2020, on = 'lib', how = 'outer')\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.merge(df_count_data_viz_li_2019, on = 'lib', how = 'outer')\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.rename(columns = {'Counts_x' : '2021 ', 'Counts_y' : '2020', 'Counts': '2019'})\ndf_count_data_viz_li_2021.at[8,'lib'] = 'D3.js'\ndf_count_data_viz_li_2021.at[8,'2019'] = df_count_data_viz_li_2021.at[12,'2019']\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021[:-1]\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.set_index('lib').T\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021[::-1]\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.drop(columns = ['None'])\n\ndf_count_data_viz_li_2021_perc = df_count_data_viz_li_2021.pct_change(periods = 1)[1:]\n\n# pct_change\ndata_viz_growth = pd.DataFrame()\ndata_viz_growth['total'] = df_count_data_viz_li_2021.sum(axis=1)\ndata_viz_growth_perc = data_viz_growth.pct_change(periods = 1)\n\n\navg_growth_2020 = round(data_viz_growth_perc.iloc[1].mean(),2)\navg_growth_2021 = round(data_viz_growth_perc.iloc[2].mean(),2)\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(22,8))\n\nplt.subplot(121)   #  subplot 1\n\ndf_count_data_viz_li_2021.plot(kind='bar', ax = ax1)\nax1.set(title = \"Visualization Library usage by professionals in kaggle survey\")\nax1.legend(loc=2, prop={'size': 9})\n\nfig.subplots_adjust(wspace=0.2)\n\n\nplt.subplot(122)   #  subplot 2\n\ndf_count_data_viz_li_2021_perc.plot(kind = 'bar', ax=ax2)\nax2.legend(loc=4, prop={'size': 9})\nplt.axhline(y=0, xmin=-1, xmax= 2, color='red', linestyle='dotted', linewidth=5)\n\nplt.axhline(y=avg_growth_2020, xmin=0.05, xmax= 0.45, color='black', linestyle='dotted', linewidth=5)\nplt.axhline(y=avg_growth_2021, xmin=0.55, xmax= 0.95, color='black', linestyle='dotted', linewidth=5)\n\nax2.text(0.26, 0.08, 'Average growth rate ' +str(avg_growth_2020*100) + '%')\nax2.text(0.25, 0.35, 'Average growth rate '+ str(avg_growth_2021*100) + '%')\n\nax2.set(title = \"Visualization library usage growth rate compare to previous year\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:53.210135Z","iopub.execute_input":"2021-11-23T05:00:53.210361Z","iopub.status.idle":"2021-11-23T05:00:53.861501Z","shell.execute_reply.started":"2021-11-23T05:00:53.210334Z","shell.execute_reply":"2021-11-23T05:00:53.860673Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The Business intelligence market is a duopoly of Tableau and Microsoft Power Bi. Interesting part here is the average growth printed 50% from 2020 to 2021, which is quite astonishing given that open source data visualization library growth was 38%. It is always interesting to see commercial projects outpace open source because it is often a sign that the industry is ready to get monetized.","metadata":{}},{"cell_type":"code","source":"viz_2021 = ['Q34_A_Part_1','Q34_A_Part_2',\n                    'Q34_A_Part_3','Q34_A_Part_4',\n                    'Q34_A_Part_5','Q34_A_Part_6',\n                    'Q34_A_Part_7','Q34_A_Part_8',\n                    'Q34_A_Part_9','Q34_A_Part_10',\n                    'Q34_A_Part_11','Q34_A_Part_12',\n                    'Q34_A_Part_13','Q34_A_Part_14',\n                    'Q34_A_Part_15','Q34_A_Part_16','Q34_A_OTHER']\ndf_viz_2021 = pd.Series(pros_2021[viz_2021].squeeze().values.ravel()).value_counts()\n    \ndf_count_viz_2021 = pd.DataFrame(df_viz_2021)\ndf_count_viz_2021 = df_count_viz_2021.reset_index()\ndf_count_viz_2021.columns = ['BI tools', '2021']\n\ndf_count_viz_2021 = df_count_viz_2021[1:] # Drop None\n\nviz_2020 = ['Q31_A_Part_1','Q31_A_Part_2',\n              'Q31_A_Part_3','Q31_A_Part_4',\n              'Q31_A_Part_5','Q31_A_Part_6',\n              'Q31_A_Part_7','Q31_A_Part_8',\n              'Q31_A_Part_9','Q31_A_Part_10',\n              'Q31_A_Part_11','Q31_A_Part_12',\n              'Q31_A_Part_13','Q31_A_Part_14','Q31_A_OTHER']\ndf_viz_2020 = pd.Series(pros_2020[viz_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_viz_2020 = pd.DataFrame(df_viz_2020)\ndf_count_viz_2020 = df_count_viz_2020.reset_index()\ndf_count_viz_2020.columns = ['BI tools', '2020']\n\ndf_count_viz_2020 = df_count_viz_2020[1:] # Drop None\n\nmerged_viz = df_count_viz_2021.set_index('BI tools').combine_first(df_count_viz_2020.set_index('BI tools'))\nmerged_viz = merged_viz.sort_values(by=['2021'], ascending = False)\n\n# Clean Merged\n\nmerged_viz['2021'].iloc[0] = merged_viz['2021'].iloc[0] + merged_viz['2021'].iloc[6] # Tableau + Tableau CRM\nmerged_viz['2020'].iloc[4] = merged_viz['2020'].iloc[4] + merged_viz['2020'].iloc[16] # Salesforce + Einstein Analytics\nmerged_viz['2021'].iloc[1] = merged_viz['2021'].iloc[1] + merged_viz['2021'].iloc[7]\n\nmerged_viz = merged_viz.drop(merged_viz.index[[6,7,16]])[:-1]\n\nmerged_viz = merged_viz.T\n\n# Pct Change\nmerged_viz_perc = merged_viz.pct_change(periods = 1)[1:]\n\n# pct_change\ndata_bi_growth = pd.DataFrame()\ndata_bi_growth['total'] = merged_viz.sum(axis=1)\ndata_bi_growth = data_bi_growth.pct_change(periods = 1)\n\navg_growth_merged_2021 = round(data_bi_growth.iloc[1].mean(),2) # avg growth rate\n\n# --- plot ----\n\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(22,8))\n\nplt.subplot(121)   #  subplot 1\n\nmerged_viz.plot(kind='bar', ax = ax1)\nax1.set(title = \"Commerical Business Intelligence tool usage by professionals in kaggle survey\")\nax1.legend(loc=1, prop={'size': 9})\n\nfig.subplots_adjust(wspace=0.2)\n\nplt.subplot(122)   #  subplot 2\n\nmerged_viz_perc.plot(kind='bar', ax=ax2)\nax2.set(title = \"Commerical Business Intelligence growth rate compare to previous year\")\nax2.legend(loc=1, prop={'size': 9})\n\nplt.axhline(y=avg_growth_merged_2021, xmin=0.2, xmax= 0.7, color='black', linestyle='dotted', linewidth=5)\n\nax2.text(-0.495, 0.52, 'Average growth rate ' +str(avg_growth_merged_2021*100) + '%')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:00:53.863011Z","iopub.execute_input":"2021-11-23T05:00:53.863490Z","iopub.status.idle":"2021-11-23T05:00:54.463277Z","shell.execute_reply.started":"2021-11-23T05:00:53.863446Z","shell.execute_reply":"2021-11-23T05:00:54.462386Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Here is the micro picture for BI tools. Preference on Google continues when it comes to \"Hope to get familiar in 2 years\"","metadata":{}},{"cell_type":"code","source":"special_BI_2021 = df_count_viz_2021\nspecial_BI_2021 = special_BI_2021.reset_index(drop=True)\n\n# -----\n\nmost_often_BI = pd.DataFrame(pros_2021['Q35'].value_counts()).reset_index()\nmost_often_BI = most_often_BI.rename(columns = {\"index\": \"BI tools\"})\n# -----\n\nBI_in_2_year = ['Q34_B_Part_1',\n           'Q34_B_Part_2',\n           'Q34_B_Part_3',\n           'Q34_B_Part_4',\n           'Q34_B_Part_5',\n           'Q34_B_Part_6',\n           'Q34_B_Part_7',\n           'Q34_B_Part_8',\n           'Q34_B_Part_9',\n           'Q34_B_Part_10',\n           'Q34_B_Part_11',\n           'Q34_B_Part_12',\n           'Q34_B_Part_13',\n           'Q34_B_Part_14',\n           'Q34_B_Part_15',\n           'Q34_B_Part_16',\n           'Q34_B_OTHER']\n\ndf_BI_in_2_year = pros_2021[BI_in_2_year]\n\ncount_BI_in_2_year = pd.Series(df_BI_in_2_year[BI_in_2_year].squeeze().values.ravel()).value_counts()\n\ndf_count_BI_in_2_year = pd.DataFrame(count_BI_in_2_year)\ndf_count_BI_in_2_year = df_count_BI_in_2_year.reset_index()\ndf_count_BI_in_2_year.columns = ['BI tools', 'Counts']\n\n#-----\nall_combo_2021 = special_BI_2021.merge(most_often_BI, on=\"BI tools\").merge(df_count_BI_in_2_year, on=\"BI tools\")\nall_combo_2021 = all_combo_2021.rename(columns = {'2021': 'Regularly use',\n                                'Q35': 'Most often use',\n                                'Counts': 'Hope to get familiar in 2 years'})\nall_combo_2021 = all_combo_2021.set_index('BI tools').T\n\nfig, ax = plt.subplots(figsize =(16,10))\nall_combo_2021.plot(kind='bar', ax=ax)\nax.set(title=\"2021 BI survey set\",\n      xlabel = \"Survey\",\n      ylabel = \"Counts\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:55:33.389920Z","iopub.execute_input":"2021-11-23T05:55:33.390812Z","iopub.status.idle":"2021-11-23T05:55:33.908843Z","shell.execute_reply.started":"2021-11-23T05:55:33.390769Z","shell.execute_reply":"2021-11-23T05:55:33.908224Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Survey data shows some interesting dynamics in this market. There was no particular dominant player within the big three rather in 2019, it is interesting to observe a relatively small player like Databricks competing against the three giants. Google outpaced the rest in 2020 but pushed back to the 4th in 2021. One reason behind this dynamic could be Google renamed all of its ML products into google vertex Ai in 2021, that change might have influenced the result. \n\nAgain, the appetite to familiarize Google's products hasn't changed. The interesting point from this chart is that Azure ML studio is just behind Google Cloud vertex Ai by a small margin.","metadata":{}},{"cell_type":"code","source":"# Machine learning tools\n# Google cloud Speect-to-Text, Google Cloud Natural Language, Google Cloud Vision, Google Cloud Translation are assumed under Google Cloud Machine Learning\n\n\n# 2021 Q31_A_Part_1\tQ31_A_Part_2\tQ31_A_Part_3\tQ31_A_Part_4\tQ31_A_Part_5\tQ31_A_Part_6\tQ31_A_Part_7\tQ31_A_Part_8\tQ31_A_Part_9\tQ31_A_OTHER\n# 2020 Q28_A_Part_1\tQ28_A_Part_2\tQ28_A_Part_3\tQ28_A_Part_4\tQ28_A_Part_5\tQ28_A_Part_6\tQ28_A_Part_7\tQ28_A_Part_8\tQ28_A_Part_9\tQ28_A_Part_10\n# 2019 Q32_Part_1\tQ32_Part_2\tQ32_Part_3\tQ32_Part_4\tQ32_Part_5\tQ32_Part_6\tQ32_Part_7\tQ32_Part_8\tQ32_Part_9\tQ32_Part_10\tQ32_Part_11\tQ32_Part_12\nml_product_2019 = ['Q32_Part_1','Q32_Part_2','Q32_Part_3','Q32_Part_4','Q32_Part_5','Q32_Part_6','Q32_Part_7','Q32_Part_8','Q32_Part_9','Q32_Part_10','Q32_Part_11','Q32_Part_12']\ndf_ml_product_2019 = pros_2019[ml_product_2019]\ncount_ml_product_2019 = pd.Series(df_ml_product_2019[ml_product_2019].squeeze().values.ravel()).value_counts()\n#count_ml_product_2019\n\ndf_count_ml_2019 = pd.DataFrame(count_ml_product_2019)\ndf_count_ml_2019 = df_count_ml_2019.reset_index()\ndf_count_ml_2019.columns = ['ML engine', 'Counts']\n\n#-------\n\nml_product_2020 = ['Q28_A_Part_1','Q28_A_Part_2','Q28_A_Part_3','Q28_A_Part_4','Q28_A_Part_5','Q28_A_Part_6','Q28_A_Part_7','Q28_A_Part_8','Q28_A_Part_9','Q28_A_Part_10']\ndf_ml_product_2020 = pros_2020[ml_product_2020]\ncount_ml_product_2020 = pd.Series(df_ml_product_2020[ml_product_2020].squeeze().values.ravel()).value_counts()\n#count_ml_product_2020\n\ndf_count_ml_2020 = pd.DataFrame(count_ml_product_2020)\ndf_count_ml_2020 = df_count_ml_2020.reset_index()\ndf_count_ml_2020.columns = ['ML engine', 'Counts']\n\n#-------\n\nml_product_2021 = ['Q31_A_Part_1','Q31_A_Part_2','Q31_A_Part_3','Q31_A_Part_4','Q31_A_Part_5','Q31_A_Part_6','Q31_A_Part_7','Q31_A_Part_8','Q31_A_Part_9','Q31_A_OTHER']\ndf_ml_product_2021 = pros_2021[ml_product_2021]\ncount_ml_product_2021 = pd.Series(df_ml_product_2021[ml_product_2021].squeeze().values.ravel()).value_counts()\n#count_ml_product_2021\n\ndf_count_ml_2021 = pd.DataFrame(count_ml_product_2021)\ndf_count_ml_2021 = df_count_ml_2021.reset_index()\ndf_count_ml_2021.columns = ['ML engine', 'Counts']\n\n\n# --------------\n\n# Products are merged into one big category. \n# ex1) google vision, google NLP - > Google Cloud Vertex AI\n# ex2) Amazon forecast, recognition - > Amazon SageMaker\n\ndf_count_ml_2019 = df_count_ml_2019.drop(df_count_ml_2019.index[[0,5,7,8,9,11]])\n#df_count_ml_2019\n\ndf_count_ml_2020 = df_count_ml_2020.drop(df_count_ml_2020.index[[0,4,5,6,7,8,9]])\n#df_count_ml_2020\n\ndf_count_ml_2021 = df_count_ml_2021.drop(df_count_ml_2021.index[[0, 7]])\n#df_count_ml_2021\n\nengine_df = df_count_ml_2021.merge(df_count_ml_2020, on = 'ML engine', how = 'outer').merge(df_count_ml_2019, how = 'outer')\nengine_df = engine_df.rename(columns = {'Counts_x' : '2021', 'Counts_y' : '2020', 'Counts' : '2019'})\nengine_df['ML engine'] = engine_df['ML engine'].str.strip()\nengine_df.at[8, 'ML engine'] = 'Google Cloud Vertex AI'\nengine_df.at[9, 'ML engine'] = 'Google Cloud Vertex AI'\nengine_df = engine_df.groupby(['ML engine']).sum()\nengine_df = engine_df.sort_values(by=['2021'], ascending = False).reset_index()\n\nengine_df = engine_df.set_index('ML engine').T[::-1]\ncolumns_clean = ['Amazon SageMaker','Azure Machine Learning Studio','Databricks','Google Cloud Vertex AI','DataRobot','Rapidminer','Alteryx','Dataiku']\nengine_df = engine_df[columns_clean]\n\ndummy_df = engine_df[['Amazon SageMaker','Azure Machine Learning Studio','Google Cloud Vertex AI']]\ndummy_df['multiplier'] = None\ndummy_df['multiplier'] = dummy_df.sum(axis=1).pct_change(periods = 1)\ngrowth_multiplier_2020 = dummy_df['multiplier'].iloc[1]\ngrowth_multiplier_2021 = dummy_df['multiplier'].iloc[2]\n\nfor col in ['Databricks', 'DataRobot', 'Rapidminer', 'Alteryx', 'Dataiku']:\n    engine_df[col].iloc[1] = engine_df[col].iloc[2]/(1+growth_multiplier_2021)\n    engine_df[col].iloc[0] = engine_df[col].iloc[1]/(1+growth_multiplier_2020)\n\nengine_df = engine_df.round(0)\n\n# color_dict = {'Databricks': '#FF0000', 'Rapidminer': '#0000FF'}\n# engine_df.plot(kind='bar', ax=ax1, color = [color_dict.get(x, '#333333') for x in engine_df.columns])\n\n\n# market share\n\nfor_perc_engine = engine_df\nfor_perc_engine = for_perc_engine.divide(for_perc_engine.sum(axis=1), axis = 0)\n\nfor_perc_engine = for_perc_engine.round(2)\n\n# future usage in 2 years\n\nmanaged_ml_2021_2_year = ['Q31_B_Part_1','Q31_B_Part_2',\n                          'Q31_B_Part_3','Q31_B_Part_4',\n                          'Q31_B_Part_5','Q31_B_Part_6',\n                          'Q31_B_Part_7','Q31_B_Part_8',\n                          'Q31_B_Part_9','Q31_B_OTHER']\n\ndf_managed_ml_2021_2_year = pros_2021[managed_ml_2021_2_year]\n\ncount_managed_ml_2021_2_year = pd.Series(df_managed_ml_2021_2_year[managed_ml_2021_2_year].squeeze().values.ravel()).value_counts()\n\ndf_count_managed_ml_2021_2_year = pd.DataFrame(count_managed_ml_2021_2_year)\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.reset_index()\ndf_count_managed_ml_2021_2_year.columns = ['Managed ML', '2021']\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.set_index('Managed ML').T\ndf_count_managed_ml_2021_2_year.columns = df_count_managed_ml_2021_2_year.columns.str.strip()\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year[['Amazon SageMaker','Azure Machine Learning Studio',\n                                 'Google Cloud Vertex AI','Databricks', 'DataRobot', 'Rapidminer', 'Alteryx', 'Dataiku']]\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.T.sort_values(by= '2021', ascending = False).T\n\n\n\n# ----- plot ------\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (22,8))\n\nplt.subplot(131)   #  subplot 1\n\nengine_df.plot(kind='bar', ax=ax1)\n\nplt.annotate('* The number of Databricks, Datarobot, RapidMiner, Alteryx and Dataiku in 2019 and 2020 were derived from the yearly avg industry growth', \n             (0,0), \n             (-50,-50), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\n\nplt.annotate(\"* Google vertex AI is launched in 2021. Google vertex AI in 2019 and 2020 is the sum of google's listed products. This applies to AWS and Azure too\", \n             (0,0), \n             (-50,-60), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\n\n\nax1.set(title = \"Managed ML product usage on regular basis\")\nax1.legend(loc='upper left', prop={'size': 9})\n\nax1.annotate('Google pushed back', xy=(1.85, 600), xytext=(0.06, 730),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )\n\n\nplt.subplot(132)   #  subplot 2\n\nfor_perc_engine.plot(kind='area', \n              stacked=True,\n              ax = ax2,\n             )\nax2.legend(loc='lower right', prop={'size': 9})\nax2.set(title = \"Share of managed ML product usage\")\n\nax2.annotate('Google loses shares', xy=(1.6, 0.72), xytext=(0.06, 0.65),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )\n\nplt.subplot(133)   #  subplot 3\n\ndf_count_managed_ml_2021_2_year.plot(kind='bar', ax = ax3,\n                                    color=['#d62728','#ff7f0e','#1f77b4','#2ca02c','#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nax3.legend(loc='lower right', bbox_to_anchor=(1.36, 0), prop={'size': 9})\nax3.set(title = \"Willingness to become more familiar in the next 2 years\",\n       xlabel = 'Year')\n\nax3.annotate('Google is back to 1st', xy=(-0.1, 1800), xytext=(0.06, 2000),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12,\n            )\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:20:53.470536Z","iopub.execute_input":"2021-11-23T05:20:53.471135Z","iopub.status.idle":"2021-11-23T05:20:54.347211Z","shell.execute_reply.started":"2021-11-23T05:20:53.471083Z","shell.execute_reply":"2021-11-23T05:20:54.346392Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"The autoML market is led by Google. It is good to see another relatively small player like H2O and DataRobot compete against the big three giants. ","metadata":{}},{"cell_type":"code","source":"\n# AutoML regular basis\n\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', 100)\n\n# AutoML regular basis\n\n# Q37_A_Part_1\tQ37_A_Part_2\tQ37_A_Part_3\tQ37_A_Part_4\tQ37_A_Part_5\tQ37_A_Part_6\tQ37_A_Part_7\tQ37_A_OTHER\n# Q34_A_Part_1\tQ34_A_Part_2\tQ34_A_Part_3\tQ34_A_Part_4\tQ34_A_Part_5\tQ34_A_Part_6\tQ34_A_Part_7\tQ34_A_Part_8\tQ34_A_Part_9\tQ34_A_Part_10\tQ34_A_Part_11\tQ34_A_OTHER\t\n# autoML_2019 = ['Q33_Part_1','Q33_Part_2','Q33_Part_3','Q33_Part_4','Q33_Part_5','Q33_Part_6','Q33_Part_7','Q33_Part_8','Q33_Part_9','Q33_Part_10','Q33_Part_11','Q33_Part_12']\n\nautoML_2021 = ['Q37_A_Part_1','Q37_A_Part_2','Q37_A_Part_3','Q37_A_Part_4','Q37_A_Part_5','Q37_A_Part_6','Q37_A_Part_7','Q37_A_OTHER']\ndf_autoML_2021 = pros_2021[autoML_2021]\ncount_autoML_2021 = pd.Series(df_autoML_2021[autoML_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2021 = pd.DataFrame(count_autoML_2021)\ndf_count_autoML_2021 = df_count_autoML_2021.reset_index()\ndf_count_autoML_2021.columns = ['AutoML', 'Counts']\n#df_count_autoML_2021\n\n#-------\n\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', 100)\n\nautoML_2020 = ['Q34_A_Part_1','Q34_A_Part_2','Q34_A_Part_3','Q34_A_Part_4','Q34_A_Part_5','Q34_A_Part_6','Q34_A_Part_7','Q34_A_Part_8','Q34_A_Part_9','Q34_A_Part_10','Q34_A_Part_11','Q34_A_OTHER']\ndf_autoML_2020 = pros_2020[autoML_2020]\ncount_autoML_2020 = pd.Series(df_autoML_2020[autoML_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2020 = pd.DataFrame(count_autoML_2020)\ndf_count_autoML_2020 = df_count_autoML_2020.reset_index()\ndf_count_autoML_2020.columns = ['AutoML', 'Counts']\n#df_count_autoML_2020\n\n# ---------------\n\nautoML_2019 = ['Q33_Part_1','Q33_Part_2','Q33_Part_3','Q33_Part_4','Q33_Part_5','Q33_Part_6','Q33_Part_7','Q33_Part_8','Q33_Part_9','Q33_Part_10','Q33_Part_11','Q33_Part_12']\ndf_autoML_2019 = pros_2019[autoML_2019]\ncount_autoML_2019 = pd.Series(df_autoML_2019[autoML_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2019 = pd.DataFrame(count_autoML_2019)\ndf_count_autoML_2019 = df_count_autoML_2019.reset_index()\ndf_count_autoML_2019.columns = ['AutoML', 'Counts']\n\n# --------------\n\nautoML_df = df_count_autoML_2021.merge(df_count_autoML_2020, on = 'AutoML', how = 'outer').merge(df_count_autoML_2019, how = 'outer')\nautoML_df = autoML_df.rename(columns = {'Counts_x' : '2021', 'Counts_y' : '2020', 'Counts' : '2019'})\nautoML_df = autoML_df.drop(autoML_df.index[[0,7,8,9,10,12,13,14,15]]) # Get rid of None and Others, auto sklearn, keras, autoML, Tpot,MLbox, Xcessiv\nautoML_df.at[16, 'AutoML'] = 'Google Cloud AutoML'\nautoML_df.at[11, 'AutoML'] = 'H2O Driverless AI'\nautoML_df['AutoML'] = autoML_df['AutoML'].str.strip()\nautoML_df = autoML_df.groupby(['AutoML']).sum()\nautoML_df = autoML_df.sort_values(by=['2021'], ascending = False)\n\nautoML_df = autoML_df.T\nautoML_df = autoML_df[::-1]\n\ndummy_df = autoML_df[['DataRobot AutoML','Databricks AutoML','Google Cloud AutoML','H2O Driverless AI']]\ndummy_df['total'] = dummy_df.sum(axis=1)\ndummy_df['total'] = dummy_df['total'].pct_change(periods=1)\nautoML_growth_2020 = dummy_df['total'].iloc[1]\nautoML_growth_2021 = dummy_df['total'].iloc[2]\n\nfor col in ['Amazon Sagemaker Autopilot', 'Azure Automated Machine Learning']:\n    autoML_df[col].iloc[1] = autoML_df[col].iloc[2]/(1+autoML_growth_2021)\n    autoML_df[col].iloc[0] = autoML_df[col].iloc[1]/(1+autoML_growth_2020)\n\nautoML_df = autoML_df.round(0)\n\n# market share\n\nfor_perc_autoML_df = autoML_df\nfor_perc_autoML_df = for_perc_autoML_df.divide(for_perc_autoML_df.sum(axis=1), axis = 0)\n\nfor_perc_autoML_df = for_perc_autoML_df.round(2)\n\n# future usage in 2 years\n\nmanaged_ml_2021_2_year = ['Q31_B_Part_1','Q31_B_Part_2',\n                          'Q31_B_Part_3','Q31_B_Part_4',\n                          'Q31_B_Part_5','Q31_B_Part_6',\n                          'Q31_B_Part_7','Q31_B_Part_8',\n                          'Q31_B_Part_9','Q31_B_OTHER']\n\nautoml_in_2 = ['Q37_B_Part_1',\n               'Q37_B_Part_2',\n               'Q37_B_Part_3',\n               'Q37_B_Part_4',\n               'Q37_B_Part_5',\n               'Q37_B_Part_6',\n               'Q37_B_Part_7',\n               'Q37_B_OTHER']\n\ndf_automl_in_2 = pros_2021[automl_in_2]\n\ncount_df_automl_in_2 = pd.Series(df_automl_in_2[automl_in_2].squeeze().values.ravel()).value_counts()\n\ndf_count_df_automl_in_2 = pd.DataFrame(count_df_automl_in_2)\ndf_count_df_automl_in_2 = df_count_df_automl_in_2.reset_index()\ndf_count_df_automl_in_2.columns = ['Auto ML', '2021']\ndf_count_df_automl_in_2 = df_count_df_automl_in_2.set_index('Auto ML').T\ndf_count_df_automl_in_2.columns = df_count_df_automl_in_2.columns.str.strip()\ndf_count_df_automl_in_2 = df_count_df_automl_in_2[['Google Cloud AutoML','Azure Automated Machine Learning', 'Amazon Sagemaker Autopilot', 'Databricks AutoML', 'DataRobot AutoML', 'H2O Driverless AI']]\n\n\n# --- Plot ----\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (22,8))\n\nplt.subplot(131)   #  subplot 1\nautoML_df.plot(kind='bar', ax=ax1)\n\nplt.annotate('* Numbers in Amazon Sagemaker Autopilot and Azure Automated Machine Learning in 2019 and 2020 were derived from the yearly avg industry growth', \n             (0,0), \n             (-50,-50), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\nax1.set(title = \"AutoML usage from 2019 to 2021\")\n\nax1.text(-0.2, 510, 'Industry dip in 2020 then bounced off',\n        verticalalignment='bottom',\n        fontsize=12)\n\n\n\nplt.subplot(132)\n\nfor_perc_autoML_df.plot(kind='area', \n              stacked=True,\n              ax = ax2,\n             )\n\nax2.legend(loc='lower right', prop={'size': 9})\nax2.set(title = \"Share of AutoML product usage\")\n\nax2.text(0.1, 0.5, 'AWS, Azure not as dominant as in other services',\n        verticalalignment='bottom',\n        fontsize=12)\n\nplt.subplot(133)\n\ndf_count_df_automl_in_2.plot(kind='bar',ax=ax3)\nax3.legend(loc='upper right', prop={'size': 9})\nax3.set(title = \"Willingness to become more familiar in the next 2 years\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:39:09.874060Z","iopub.execute_input":"2021-11-23T05:39:09.874403Z","iopub.status.idle":"2021-11-23T05:39:10.587825Z","shell.execute_reply.started":"2021-11-23T05:39:09.874364Z","shell.execute_reply":"2021-11-23T05:39:10.586816Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Let's dive further into how the cloud market will evolve along with survey data. First, the Cloud industry is spending a hell of money on data science. The amount of money NVIDIA is collecting from the data center is amazing. NVIDIA generated $6.7 bln of revenue in 2020. That being said, the big three spent at least $3.8 bln on GPUs. This trend will accelerate further along with data science.","metadata":{}},{"cell_type":"code","source":"nvda_earnings = pd.read_excel('../input/nvda-earnings-2018q1to2021q3/nvda_earnings.xlsx')\nnvda_earnings = nvda_earnings.rename(columns = {'Unnamed: 0': 'Revenue'})\n\ndf = nvda_earnings.T\ndf.columns = df.iloc[0]\ndf = df.drop(df.index[0])\ndf.drop('Total', axis = 1, inplace = True)\n\n# -------------\n\ndata_center_segment = nvda_earnings.iloc[[2]]\ndata_center_segment = data_center_segment.set_index(['Revenue'])\ndata_center_aws =  data_center_segment.copy() * 0.31\ndata_center_aws = data_center_aws.rename(index = {'data center': 'Aws'})\n\ndata_center_azure = data_center_segment.copy() * 0.22\ndata_center_azure = data_center_azure.rename(index = {'data center': 'Azure'})\n\ndata_center_gcp = data_center_segment.copy() * 0.08\ndata_center_gcp = data_center_gcp.rename(index = {'data center': 'GCP'})\n\ndata_center_others = data_center_segment.copy() * 0.39\ndata_center_others = data_center_others.rename(index = {'data center': 'Others'})\n\ndf_data_center_others = pd.concat([data_center_aws, data_center_azure, data_center_gcp, data_center_others])\ndf_data_center_others.iloc[2, 0] = 0\ndf_data_center_others.iloc[2, 1] = 0\ndf_data_center_others.iloc[2, 2] = 0 \ndf_data_center_others = df_data_center_others.T\n\n\n\n# ---------------\n\nfig, (ax1,ax2) = plt.subplots(ncols = 2, figsize = (20,8))\n\nplt.subplot(121)   #  subplot 1\n\ndf.plot(kind = 'bar', stacked=True, ax = ax1)\nax1.set(title = 'NVIDIA Earnings')\nfor c in ax1.containers:\n    ax1.bar_label(c, label_type='center')\n\n#for p in ax.patches:\n    #print(p)\n\nax1.legend(loc='upper left',\n          ncol=1, fancybox=True, shadow=True)\n\nax1.set_xlabel(\"Adj Fiscal Year\")\nax1.set_ylabel(\"in Million USD\")\n\nplt.annotate('',\nha = 'center', va = 'bottom',\nxytext = (7.8, 1800),\nxy = (12.3, 3800),\narrowprops = { 'facecolor' : 'red', 'shrink' : 0.05 })\n\nplt.annotate('Data Center Revenue Explosion Begins',\n        fontsize = 14,\n        ha = 'center', va = 'bottom',\n        xytext = (8, 5000),\n        xy = (8, 3200),\n        arrowprops = { 'facecolor' : 'black', 'shrink' : 0.05 })\n\n\n#plt.annotate('Data Center revenue growth 100% YoY in 2020', xytext = (3, 6000), xy = (8, 6000), fontsize = 22, color = 'red', )\nplt.axhline(y=3200, linestyle='dashed', linewidth=2, color = 'black') \nplt.annotate('Source: Nvidia, Amazon, Google, Microsoft IR', (0,0), (-80,-80), fontsize=8, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.subplot(122)   #  subplot 1\n\n\n\ndf_data_center_others.plot(kind = 'bar', ax = ax2)\nplt.axvline(x=2.5, linestyle='dashed', linewidth=2, color = 'black') \nplt.annotate('Google disclosed GCP as of 2019', xytext = (2.6, 600), xy = (8, 600), fontsize = 14, color = 'black' )\nax2.set(title = \"Estimated Nvidia' data center revenue from the big 3\")\n\nax2.legend(loc='upper left',\n          ncol=1, fancybox=True, shadow=True)\n\nax2.set_xlabel(\"Adj Fiscal Year\")\nax2.set_ylabel(\"in Million USD\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:06:26.728623Z","iopub.execute_input":"2021-11-23T06:06:26.729184Z","iopub.status.idle":"2021-11-23T06:06:27.941125Z","shell.execute_reply.started":"2021-11-23T06:06:26.729147Z","shell.execute_reply":"2021-11-23T06:06:27.940524Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n    \n**Exhibit 1: Data Science Professionals distribution by industry 2018 vs. 2021**\n</div>","metadata":{}},{"cell_type":"code","source":"# Exhibit 1. Data Science Professionals distribution by industry 2018 vs. 2021\n\n# Get data from 2021\n\nindustry_2021 = data_2021[data_2021['Q20'].notna()]\nc = industry_2021['Q20'].value_counts(normalize=True).rename_axis('industry').reset_index(name='counts')\n#c =industry_2021['Q20'].value_counts().rename_axis('industry').reset_index(name='counts')\n\n# Get data from 2018\n\nindustry_2018 = data_2018[data_2018['Q7'] != 'I am a student']\nindustry_2018 = industry_2018[industry_2018['Q7'].notna()]\nd = industry_2018['Q7'].value_counts(normalize=True).rename_axis('industry').reset_index(name='counts')\n#d = industry_2018['Q7'].value_counts().rename_axis('industry').reset_index(name='counts')\n\n# compute the industry\n\nk = pd.merge(left = d, right = c, on = 'industry')\nk = k.rename(columns = {'counts_x': '2018', 'counts_y': '2021'})\nk = k.sort_values(by=['2021'], ascending=False)\n\n# compute the difference\ndiff_industry = k.copy()\ndiff_industry['dff'] = k['2021'] - k['2018']\n\n# plot\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (16,8))\n#plt.style.use('IPython_default')\ngs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \nax1 = plt.subplot(gs[0])\n\nk.plot.barh(x = \"industry\", ax= ax1)\n#ax.grid(False)\nax1.set(title = \"Data Science professionals distribution by Industry. 2018 vs 2021\",\n      xlabel = \"Percentage\",\n      ylabel = \"Industry\")\nax1.invert_yaxis()\n\nax2 = plt.subplot(gs[1])\ndiff_industry['dff'].plot(kind='barh', x = 'industry', ax = ax2,\n                    color=(diff_industry['dff'] > 0).map({True: 'g',\n                                                    False: 'r'}))\nax2.set(title = \"Change\",\n      xlabel = \"Percentage\",\n      ylabel = \"Industry\")\nax2.set_yticks([])\n\nplt.gca().invert_yaxis()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-23T05:00:58.137826Z","iopub.execute_input":"2021-11-23T05:00:58.138041Z","iopub.status.idle":"2021-11-23T05:00:58.984981Z","shell.execute_reply.started":"2021-11-23T05:00:58.138016Z","shell.execute_reply":"2021-11-23T05:00:58.984147Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n\n<ul>\n<li> Computer/Technology field is no longer the only game in town. It declined <strong>7.5%</strong></li>\n\n<li> Academic/Education industry gained <strong>3%</strong></li>\n<li> Manufactruing industry gained <strong>2%</strong></li>\n\n<li> A sign that Data Science is <strong>penetrating broadly</strong>. </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# ----------------\n# Job title filter\n# ----------------\n\njob_title = {'Other':'Other',\n     'Product Manager': 'Product/Project Manager',\n 'Program/Project Manager':'Product/Project Manager',\n 'Principal Investigator':'Product/Project Manager',\n 'Chief Officer':'Product/Project Manager',\n 'Manager':'Product/Project Manager',\n 'Software Developer/Software Engineer': 'Software Engineer',\n 'Operations Research Practitioner': 'Research Scientist',\n 'Computer Scientist': 'Research Scientist',\n 'Scientist/Researcher': 'Research Scientist',\n 'Researcher': 'Research Scientist',\n 'Data Scientist': 'Data Scientist',\n     'Business Analyst': 'Business Analyst',\n     'Engineer': 'Other',\n     'DBA/Database Engineer': 'DBA/Database Engineer',\n     'Data Analyst':'Data Analyst',\n     'Machine Learning Engineer': 'Machine Learning Engineer',\n     'Statistician':'Statistician',\n     'Predictive Modeler':'Research Scientist',\n     'Programmer': 'Software Engineer',\n     'Data Miner': 'Data Engineer',\n     'Consultant': 'Other',\n     'Research Assistant': 'Research Scientist',\n     'Chief Officer':'Product/Project Manager',\n     'Data Engineer':'Data Engineer',\n     'Developer Advocate': 'Developer Relations/Advocacy',\n     'Marketing Analyst': 'Business Analyst',\n     'Data Analyst': 'Data Analyst',\n     'Software Engineer': 'Software Engineer',\n     'Research Scientist': 'Research Scientist',\n     'Data Journalist': 'Data Analyst',\n     'Salesperson':'Developer Relations/Advocacy',\n     'Product/Project Manager': 'Product/Project Manager',\n     'Developer Relations/Advocacy': 'Developer Relations/Advocacy'\n}","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-23T05:00:58.986427Z","iopub.execute_input":"2021-11-23T05:00:58.987245Z","iopub.status.idle":"2021-11-23T05:00:58.995181Z","shell.execute_reply.started":"2021-11-23T05:00:58.987190Z","shell.execute_reply":"2021-11-23T05:00:58.994383Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n    \n**Exhibit 1.1: Data Science Professional roles in different industry 2018 vs. 2021**\n</div>","metadata":{}},{"cell_type":"code","source":"# cheat\npd.options.mode.chained_assignment = None \n\n# -----------------------------------------\n# Heatmap of job title within industry 2021\n# -----------------------------------------\n\nworkforce_2021 = get_professionals(data_2021, 'Q5')\nprofessional_2021 = workforce_2021[workforce_2021['Q20'].notna()]\nprofessional_2021['Q5'] = professional_2021['Q5'].map(job_title)\nindustry_2021 = professional_2021['Q20'].unique()\ndf_2021 = professional_2021[['Q5','Q20']]\n\ntemp_d = {}\n\nfor industry in industry_2021:\n    temp_df = df_2021[df_2021['Q20'] == industry]\n    temp_dict = dict(temp_df['Q5'].value_counts())\n    temp_d[industry] = temp_dict\n\ndef sorted_simple_dict(d):\n    return {k: v for k, v in sorted(d.items())}\n\ndef sorted_once_nested_dict(d):\n    return {k: sorted_simple_dict(v) for k, v in sorted(d.items())}\n\ntemp_d = sorted_once_nested_dict(temp_d)\n\nd_2021 = {}\n\nh_lst = list(k['industry'])\n\nfor i in h_lst:\n    d_2021[i] = temp_d[i]\n\ndf_industry_2021 = pd.DataFrame.from_dict(d_2021, orient='index')\ndf_industry_2021.fillna(0, inplace = True)\ndf_industry_2021 = df_industry_2021.sort_values(by=df_industry_2021.index[0], ascending=False, axis=1)\ndf_industry_2021\n\n# -----------------------------------------\n# Heatmap of job title within industry 2021\n# -----------------------------------------\n\nstudent_2018 = data_2018.loc[data_2018['Q7'] == 'I am a student']\nworkforce_2018 = data_2018.loc[data_2018['Q7'] != 'I am a student']\n\nprofessional_2018 = workforce_2018[workforce_2018['Q7'].notna()]\nprofessional_2018['Q6'] = professional_2018['Q6'].map(job_title)\n\nindustry_2018 = professional_2018['Q7'].unique()\n\ndf_2018 = professional_2018[['Q6','Q7']]\n\ntemp_d = {}\n\nfor industry in industry_2018:\n    temp_df = df_2018[df_2018['Q7'] == industry]\n    temp_dict = dict(temp_df['Q6'].value_counts())\n    temp_d[industry] = temp_dict\n\ndef sorted_simple_dict(d):\n    return {k: v for k, v in sorted(d.items())}\n\ndef sorted_once_nested_dict(d):\n    return {k: sorted_simple_dict(v) for k, v in sorted(d.items())}\n\ntemp_d = sorted_once_nested_dict(temp_d)\n\nd_2018 = {}\n\nfor i in h_lst:\n    d_2018[i] = temp_d[i]\n\ndf_industry_2018 = pd.DataFrame.from_dict(d_2018, orient='index')\ndf_industry_2018.fillna(0, inplace = True)\ndf_industry_2018 = df_industry_2018.sort_values(by=df_industry_2018.index[0], ascending=False, axis=1)\n#df_industry_2018\n\n# ---------------\n#  SUBPLOTS - 1x2\n# ---------------\n\nfig = plt.figure(figsize=(22,10))\n\nplt.subplot(121)   #  subplot 1\nplt.title('2018 heatmap')\nsns.heatmap(df_industry_2018, annot=True, annot_kws = {\"size\": 8}, linewidth = 0.5, fmt='g', square=True, cmap = 'BuGn')\n\nfig.subplots_adjust(wspace=0.4)\n\nplt.subplot(122)   #  subplot 2\nplt.title('2021 heatmap')\nsns.heatmap(df_industry_2021, annot=True, annot_kws = {\"size\": 8}, linewidth = 0.5, fmt='g', square=True, cmap = 'BuGn')\n\nplt.axvline(x = 2, ymin= -0.05, ymax= 1.1, color='red', linestyle='solid', linewidth=5)\nplt.axvline(x = 3, ymin= -0.05, ymax= 1.1, color='red', linestyle='solid', linewidth=5)\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-23T05:00:58.996765Z","iopub.execute_input":"2021-11-23T05:00:58.997054Z","iopub.status.idle":"2021-11-23T05:01:02.287691Z","shell.execute_reply.started":"2021-11-23T05:00:58.997027Z","shell.execute_reply":"2021-11-23T05:01:02.286876Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>Machine Learning Engineer role is added in 2021. <strong>Large portion of data scientist and software engineer moved to machine learning engineer.</strong></li>\n    \n<li><strong>Decline in Research Scientist roles</strong> in Computer/Tech and Academic fields.</li>\n</ul>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom math import pi\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# activities at work\n\n# Q24_Part_1\tQ24_Part_2\tQ24_Part_3\tQ24_Part_4\tQ24_Part_5\tQ24_Part_6\tQ24_Part_7\tQ24_OTHER\n\nactivities_2021 = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\n\nrole_2021 = ['Q5']\n\nrole_activities_2021 = ['Q5','Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\n\njob_name = ['Data Scientist', 'Machine Learning Engineer',  'Software Engineer', 'Data Analyst', 'Research Scientist']\n\ndf_role_act_2021 = pros_2021[role_activities_2021]\n\ndf_role_act_2021 = df_role_act_2021[df_role_act_2021['Q5'].isin(job_name)]\n\ndf_role_act_2021[activities_2021] = df_role_act_2021[activities_2021].notnull().astype('int')\n\n# --------------\n\ndf_SE = df_role_act_2021[df_role_act_2021['Q5'] == 'Software Engineer']\ndf_SE = df_SE.groupby(by='Q5', dropna=False).sum()\ndf_SE = df_SE.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\n\ndf_SE = df_SE.T.reset_index()\n\n# --------------\n\ndf_DS = df_role_act_2021[df_role_act_2021['Q5'] == 'Data Scientist']\ndf_DS = df_DS.groupby(by='Q5', dropna=False).sum()\ndf_DS = df_DS.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\ndf_DS = df_DS.T.reset_index()\n\n# -------------\n\ndf_MLE = df_role_act_2021[df_role_act_2021['Q5'] == 'Machine Learning Engineer']\n#print(df)\ndf_MLE = df_MLE.groupby(by='Q5', dropna=False).sum()\ndf_MLE = df_MLE.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\ndf_MLE = df_MLE.T.reset_index()\n\n#----------\n\n\n\ndf_MLE['percent'] = (df_MLE['Machine Learning Engineer'] / \n                  df_MLE['Machine Learning Engineer'].sum()) * 100\ndf_MLE = df_MLE.drop('Machine Learning Engineer', axis = 1)\n\ndf_DS['percent'] = (df_DS['Data Scientist'] / \n                  df_DS['Data Scientist'].sum()) * 100\ndf_DS = df_DS.drop('Data Scientist', axis = 1)\n\ndf_SE['percent'] = (df_SE['Software Engineer'] / \n                  df_SE['Software Engineer'].sum()) * 100\ndf_SE = df_SE.drop('Software Engineer', axis = 1)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatterpolar(\n      r=df_SE['percent'],\n      theta=df_SE['index'],\n      fill='toself',\n      name='Software Engineer'\n))\n\n\nfig.add_trace(go.Scatterpolar(\n      r=df_DS['percent'],\n      theta=df_DS['index'],\n      fill='toself',\n      name='Data Scientist'\n))\n\nfig.add_trace(go.Scatterpolar(\n      r=df_MLE['percent'],\n      theta=df_MLE['index'],\n      fill='toself',\n      name='Machine Learning Engineer'\n))\n\n\nfig.update_layout(\n     title={\n        'text': \"Tasks in workplace SE vs. DS vs. MLE\",\n        'y':0.92,\n        'x':0.45,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n  polar=dict(\n    radialaxis=dict(\n      visible=True,\n      range=[0, 27]\n    )),\n  showlegend=True\n)\n\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-23T05:01:02.288963Z","iopub.execute_input":"2021-11-23T05:01:02.289230Z","iopub.status.idle":"2021-11-23T05:01:03.456888Z","shell.execute_reply.started":"2021-11-23T05:01:02.289190Z","shell.execute_reply":"2021-11-23T05:01:03.456107Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>Machine learning engineer's tasks are hybrid of traditional software engineer and data scientist. <strong>MLEs conduct more resources on managing products than analyzing data or building infrastructure.</strong></li>\n    <li>The rise of Machine learning enginner and decline in research scientist in academic field suggests that <strong>the data science industry is moving from research to business/operation oriented</strong></li>\n\n</ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Exhibit 1.1 Data Science distribution by company size 2021.\n\ntest = get_professionals(data_2021, 'Q5')\n#print(len(test))\ntest = test[test['Q21'].notna()]\n#print(len(test))\n\ncategory_test = test.groupby(['Q20', 'Q21']).size()\n#category_test.plot(kind='bar')\nnew_df = category_test.to_frame(name = 'size').reset_index()\nnew_df_2= pd.pivot(\n    data = new_df,\n    index = 'Q20',\n    columns = 'Q21',\n    values = 'size')\nnew_df_2.index.names = ['Industry']\nnew_df_2.columns.names = ['Company Size']\n\ncolumns_order = ['0-49 employees', '50-249 employees', '250-999 employees', '1000-9,999 employees','10,000 or more employees']\n\nnew_df_2 = new_df_2.reindex(columns = columns_order)\nnew_df_2['total'] = new_df_2[columns_order].sum(axis = 1)\nnew_df_2 = new_df_2.sort_values(by = 'total', ascending = False)\nnew_df_2 = new_df_2.drop(columns='total')\n\n# -----\n\n\nnew_df_2['total'] = new_df_2[columns_order].sum(axis = 1)\nnew_df_2 = new_df_2.sort_values(by = 'total', ascending = False)\nnew_df_2 = new_df_2.drop(columns = 'total')\nres = new_df_2.div(new_df_2.sum(axis=1), axis = 0)\nres\n\n# -----\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (16,8))\n#plt.style.use('IPython_default')\ngs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n\nax1 = plt.subplot(gs[0])\n\nnew_df_2.plot(use_index = True,  \n              kind='barh', \n              stacked=True, \n              ax = ax1,\n              )\n\nax1.set(title = \"DS professional distibution by company size across different industry 2021\",\n      xlabel = \"Counts\",\n      ylabel = \"Industry\")\n\n\nax2 = plt.subplot(gs[1])\n\nres.plot(use_index = True,  \n              kind='barh', \n              stacked=True, \n              ax = ax2,\n              )\n\nax2.set(title='Company Size portion within industry',\n      xlabel = \"Percentage\",\n      ylabel = \" \")\n\nplt.legend(title = \"Company Size\", bbox_to_anchor=(1.04,1), loc=\"upper left\")\nax2.set_yticks([])\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-23T05:01:03.458271Z","iopub.execute_input":"2021-11-23T05:01:03.458993Z","iopub.status.idle":"2021-11-23T05:01:04.719801Z","shell.execute_reply.started":"2021-11-23T05:01:03.458958Z","shell.execute_reply":"2021-11-23T05:01:04.719201Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>The survey data suggests that Data science professionals are distributed proportionally across general industry.</li>\n    <li>Start-up (0-49 employees) accounts the most in Non-profit/Service.</li>\n    <li>Large corporation (10,000 + employees) accounts the most in Insurance/Risk Assessment </li>\n</ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"test = get_professionals(data_2021, 'Q5')\ntest = test[test['Q26'].notna()]\ntest['Q26'].unique()\n#test = test[test['Q26'] != '$0 ($USD)']\ntest = test.groupby(['Q26', 'Q20']).size()\ndf = test.to_frame(name = 'size').reset_index()\ndf= pd.pivot(\n    data = df,\n    index = 'Q20',\n    columns = 'Q26',\n    values = 'size')\ndf.index.names = ['Industry']\ndf.columns.names = ['Money Spent']\n\n#c = ['$0 ($USD)', '$1-$99','$100-$999', '$1000-$9,999','$10,000-$99,999', '$100,000 or more ($USD)']\nc = ['$1-$99','$100-$999', '$1000-$9,999','$10,000-$99,999', '$100,000 or more ($USD)']\ndf = df.reindex(c, axis = 1)\ndf = df.sort_values(by='$100,000 or more ($USD)', ascending = False)\n\nfig, ax1 = plt.subplots(figsize = (21,10))\ndf.plot(kind = 'barh', ax = ax1)\nax1.set(title = \"Money Spent on ML or Cloud computing service by industry ranked in 100,000+ in 2021\",\n       xlabel = \"Counts\",\n       ylabel = \"Industry\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-23T05:01:04.720692Z","iopub.execute_input":"2021-11-23T05:01:04.721250Z","iopub.status.idle":"2021-11-23T05:01:05.888102Z","shell.execute_reply.started":"2021-11-23T05:01:04.721215Z","shell.execute_reply":"2021-11-23T05:01:05.887229Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>Computers/Technology accounts the highest number of companies that spend over $100,000+</li>\n    <li>While the number of data science professionals working in Academic/Education industry ranked the second(in previous chart), the number of institutions that spend over 100,000+ are ranked only at 6th.</li>\n\n</ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = {'$0 ($USD)': '$0 ($USD)',\n     '$1-$99': '$1-$99',\n     '$100-$999': '$100-$999',\n     '$1000-$9,999': '$1000-$9,999',\n     '$10,000-$99,999': '$10,000-$99,999',\n     '$100,000 or more ($USD)': '$100,000 or +',\n     '> $100,000 ($USD)': '$100,000 or +',\n}\n\ntemp_2021 = get_professionals(data_2021, 'Q5')\ntemp_2020 = get_professionals(data_2020, 'Q5')\n\n\ndata_2019 = data_2019[data_2019['Q5'].notna()]\ndata_2019 = data_2019[data_2019['Q5'] != 'Student']\ndata_2019 = data_2019[data_2019['Q5'] != 'Not employed']\n#data_2019['Q11'] = data_2019['Q11'].map(c)\n\nmoney_spent_2021 = temp_2021[temp_2021['Q26'].notna()]\nmoney_spent_2021['Q26'] = money_spent_2021['Q26'].map(c)\nmoney_spent_2021 = money_spent_2021[money_spent_2021['Q26'] != '$0 ($USD)']\n\nmoney_spent_2020 = temp_2020[temp_2020['Q25'].notna()]\nmoney_spent_2020['Q25'] = money_spent_2020['Q25'].map(c)\nmoney_spent_2020 = money_spent_2020[money_spent_2020['Q25'] != '$0 ($USD)']\n\nmoney_spent_2019 = data_2019[data_2019['Q11'].notna()]\nmoney_spent_2019['Q11'] = money_spent_2019['Q11'].map(c)\n\nrow_order = ['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or +']\n\ndf_2021 = pd.DataFrame(money_spent_2021['Q26'].value_counts(), index = row_order)\ndf_2020 = pd.DataFrame(money_spent_2020['Q25'].value_counts(), index = row_order)\ndf_2019 = pd.DataFrame(money_spent_2019['Q11'].value_counts(), index = row_order)\n\ndf_final = pd.concat([df_2019, df_2020, df_2021], axis = 1)\ndf_final.rename(columns = {'Q26': '2021', 'Q25': '2020', 'Q11': '2019'}, inplace= True)\ndf_final = df_final.T\n\nfig, ax1 = plt.subplots(figsize = (16,8))\ndf_final.plot(kind = 'bar', ax = ax1)\nax1.set(title = \"The amount and number of companies/institutions spending on ML infrastructure from 2019 to 2021\",\n       xlabel = \"Year\",\n       ylabel = \"Counts\")\nax1.legend(title='money spent')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:05.889472Z","iopub.execute_input":"2021-11-23T05:01:05.889746Z","iopub.status.idle":"2021-11-23T05:01:07.077345Z","shell.execute_reply.started":"2021-11-23T05:01:05.889711Z","shell.execute_reply":"2021-11-23T05:01:07.076762Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>Total aggregate spending on ML went up from 2019 to 2021. The dip in 2020 is likely due to covid-19</li>\n\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:07.078644Z","iopub.execute_input":"2021-11-23T05:01:07.079049Z","iopub.status.idle":"2021-11-23T05:01:07.461703Z","shell.execute_reply.started":"2021-11-23T05:01:07.079017Z","shell.execute_reply":"2021-11-23T05:01:07.460856Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hardware_2021 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER']\nhardware_2021_df = pros_2021[hardware_2021]\ncount_hardware_2021 = pd.Series(hardware_2021_df[hardware_2021].squeeze().values.ravel()).value_counts()\n\nhardware_2020 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_OTHER']\nhardware_2020_df = pros_2020[hardware_2020]\ncount_hardware_2020 = pd.Series(hardware_2020_df[hardware_2020].squeeze().values.ravel()).value_counts()\n\nhardware_2019 = ['Q21_Part_1','Q21_Part_2','Q21_Part_3','Q21_Part_4','Q21_Part_5','Q21_OTHER_TEXT']\nhardware_2019_df = pros_2019[hardware_2019]\ncount_hardware_2019 = pd.Series(hardware_2019_df[hardware_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_hardware_2021 = pd.DataFrame(count_hardware_2021)\ndf_count_hardware_2021 = df_count_hardware_2021.reset_index()\ndf_count_hardware_2021.columns = ['manage', '2021']\n\ndf_count_hardware_2020 = pd.DataFrame(count_hardware_2020)\ndf_count_hardware_2020 = df_count_hardware_2020.reset_index()\ndf_count_hardware_2020.columns = ['manage', '2020']\n\ndf_count_hardware_2019 = pd.DataFrame(count_hardware_2019)\ndf_count_hardware_2019 = df_count_hardware_2019.reset_index()\ndf_count_hardware_2019.columns = ['manage', '2019']\n\ndf_count_hardware_2019 = df_count_hardware_2019.iloc[1:6]\ndf_count_hardware_2019 = df_count_hardware_2019.set_index('manage').T[['GPUs', 'TPUs','Other']]\n\ndf_count_hardware_2020 = df_count_hardware_2020.set_index('manage').T[['GPUs', 'TPUs', 'Other']]\n\ndf_count_hardware_2021 = df_count_hardware_2021.set_index('manage').T\ndf_count_hardware_2021.columns = df_count_hardware_2021.columns.str.strip()\ndf_count_hardware_2021 = df_count_hardware_2021[['NVIDIA GPUs','Google Cloud TPUs','Other','AWS Inferentia Chips','AWS Trainium Chips']]\ndf_count_hardware_2021 = df_count_hardware_2021.rename(columns = {'NVIDIA GPUs': 'GPUs',\n                                                                    'Google Cloud TPUs': 'TPUs',\n                                                                    'Other': 'Other',\n                                                                    'AWS Inferentia Chips': 'AWS Inferentia',\n                                                                    'AWS Trainium Chips': 'AWS Trainium'})\n\ndf_hardware_merged = df_count_hardware_2019.append(df_count_hardware_2020).append(df_count_hardware_2021).fillna(0)\n\n\n\n# ---------------\n\nlarge_spender_2020 = pros_2020.loc[pros_2020['Q25'].isin(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)'])]\nhardware_2020 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_OTHER']\n\nspender_hardware_2020 = pd.DataFrame()\n\nfor idx, comp_size in enumerate(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)']):\n    spender_comp_size = large_spender_2020.loc[large_spender_2020['Q25'] == comp_size]\n    idx = pd.Series(spender_comp_size[hardware_2020].squeeze().values.ravel()).value_counts()\n    spender_hardware_2020[comp_size] = idx\n    \nspender_hardware_2020 = spender_hardware_2020.rename(columns = {'$100,000 or more ($USD)': '100,000 +'})\nspender_hardware_2020 = spender_hardware_2020.T[['GPUs','TPUs', 'Other']]\n\nlarge_spender_2021 = pros_2021.loc[pros_2021['Q26'].isin(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)'])]\n\nhardware_2021 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER']\n# ------------------\n\nspender_hardware_2021 = pd.DataFrame()\n\nfor idx, comp_size in enumerate(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)']):\n    spender_comp_size = large_spender_2021.loc[large_spender_2021['Q26'] == comp_size]\n    idx = pd.Series(spender_comp_size[hardware_2021].squeeze().values.ravel()).value_counts()\n    spender_hardware_2021[comp_size] = idx\n    \nspender_hardware_2021 = spender_hardware_2021.rename(columns = {'$100,000 or more ($USD)': '100,000 +'})\nspender_hardware_2021 = spender_hardware_2021[1:].T\n\n# ---- plot\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (22,8))\n\nplt.subplot(131)\ndf_hardware_merged.plot(kind='bar', ax=ax1)\nax.set(title = 'Special hardware usage trend',\n    xlabel = 'year',\n      ylabel = 'number of respondants',\n      )\n\nax.annotate(\"AWS's custom chips are added in 2021\", xy=(2.1, 1000), xytext=(2.3, 2000),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n\nax.legend(title='Special hardware')\n\n\nplt.subplot(132)\nspender_hardware_2020.plot(kind='bar', ax=ax2)\nax2.set(title = \"Special hardware usage by spending size in 2020\")\nax2.legend(loc=1, prop={'size': 9})\n\nfig.subplots_adjust(wspace=0.2)\n\nplt.subplot(133)\nspender_hardware_2021.plot(kind='bar', ax=ax3)\nax3.set(title = \"Special hardware usage by spending size in 2021\")\nax3.legend(loc=1, prop={'size': 9})","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:23:19.163168Z","iopub.execute_input":"2021-11-23T06:23:19.163454Z","iopub.status.idle":"2021-11-23T06:23:20.405011Z","shell.execute_reply.started":"2021-11-23T06:23:19.163425Z","shell.execute_reply":"2021-11-23T06:23:20.403989Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n<ul>\n    <li>Broad increase in TPU usage.</li>\n\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# ---------------------\n# ML Algos 2019 to 2021\n# ---------------------\n\n# ----\n# 2021\n# ----\ntest = get_professionals(data_2021, 'Q5')\n\nold_colnames = ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\nnew_colnames = ['Regression', 'Decision Trees', 'Gradient Boosting', 'Bayesian', 'Evolutionary', 'DNN', 'CNN', 'GAN', 'RNN', 'Transformer', 'None', 'Other']\n\nalgos = test[['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']]\ncol_rename_dict = {i:j for i,j in zip(old_colnames,new_colnames)}\nalgos.rename(columns=col_rename_dict, inplace=True)\nalgos = algos.fillna(0)\nalgos[algos != 0] = 1\nalgos.replace({False: 0, True: 1}, inplace=True)\nalgo_df = algos[algos==True].count(axis=0).rename_axis('Algo').reset_index(name='2021')\nalgo_df = algo_df.set_index('Algo').T\nstuff_2021 = algo_df\n\n# ----\n# 2020\n# ----\n\ntest = get_professionals(data_2020, 'Q5')\n\nold_colnames = ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11','Q17_OTHER']\nnew_colnames = ['Regression', 'Decision Trees', 'Gradient Boosting', 'Bayesian', 'Evolutionary', 'DNN', 'CNN', 'GAN', 'RNN', 'Transformer', 'None', 'Other']\n\nalgos = test[['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11','Q17_OTHER']]\ncol_rename_dict = {i:j for i,j in zip(old_colnames,new_colnames)}\nalgos.rename(columns=col_rename_dict, inplace=True)\nalgos = algos.fillna(0)\nalgos[algos != 0] = 1\nalgos.replace({False: 0, True: 1}, inplace=True)\nalgo_df = algos[algos==True].count(axis=0).rename_axis('Algo').reset_index(name='2020')\nalgo_df = algo_df.set_index('Algo').T\nstuff_2020 = algo_df\n\n# ----\n# 2019\n# ----\n\nold_colnames = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_Part_8','Q24_Part_9','Q24_Part_10','Q24_Part_11', 'Q24_Part_12']\n#new_colnames = ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11']\n\nnew_colnames = ['Regression', 'Decision Trees', 'Gradient Boosting', 'Bayesian', 'Evolutionary', 'DNN', 'CNN', 'GAN', 'RNN', 'Transformer', 'None', 'Other']\ncol_rename_dict = {i:j for i,j in zip(old_colnames,new_colnames)}\n\n\npd.set_option('display.max_columns', None)\ndata_2019.head()\ndata_2019 = data_2019[data_2019['Q5'].notna()]\ndata_2019 = data_2019[data_2019['Q5'] != 'Student']\ndata_2019 = data_2019[data_2019['Q5'] != 'Not employed']\nalgos = data_2019[['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_Part_8','Q24_Part_9','Q24_Part_10','Q24_Part_11','Q24_Part_12']]\ncol_rename_dict = {i:j for i,j in zip(old_colnames,new_colnames)}\nalgos.rename(columns=col_rename_dict, inplace=True)\nalgos = algos.fillna(0)\nalgos[algos != 0] = 1\nalgos.replace({False: 0, True: 1}, inplace=True)\nalgo_df = algos[algos==True].count(axis=0).rename_axis('Algo').reset_index(name='2019')\nalgo_df = algo_df.set_index('Algo').T\nstuff_2019 = algo_df\n\n# ---------------\n# Merge the frame\n# ---------------\n\nalgo_set = stuff_2021.append([stuff_2020, stuff_2019])\nalgo_set = algo_set.T\nalgo_set = algo_set[['2019', '2020', '2021']]\nalgo_set = algo_set.sort_values(by='2021',ascending = False)\n\n# -------------\n\ngpu_algo = ['Q12_Part_1',\n           'Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\n\ntpu_algo = ['Q12_Part_2',\n           'Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\n\nalgo = ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\ngpu_use_algo = pros_2021[gpu_algo]\ngpu_use_algo = gpu_use_algo[gpu_use_algo['Q12_Part_1'].notna()]\n\ngpu_algo_count = pd.Series(gpu_use_algo[algo].squeeze().values.ravel()).value_counts()\ndf_gpu_algo_count = pd.DataFrame(gpu_algo_count)\n\n\n# ---------\n\n\ngpu_algo = ['Q12_Part_1',\n           'Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\n\ntpu_algo = ['Q12_Part_2',\n           'Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\n\nalgo = ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', 'Q17_Part_8', 'Q17_Part_9','Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER']\ntpu_algo_algo = pros_2021[tpu_algo]\ntpu_algo_algo = tpu_algo_algo[tpu_algo_algo['Q12_Part_2'].notna()]\n\ntpu_algo_count = pd.Series(tpu_algo_algo[algo].squeeze().values.ravel()).value_counts()\ndf_tpu_algo_count = pd.DataFrame(tpu_algo_count)\n\n# -----------\n\ngpu_tpu_algo = pd.DataFrame()\ngpu_tpu_algo['NVIDIA GPUs'] = df_gpu_algo_count\ngpu_tpu_algo['Google Cloud TPUs'] = df_tpu_algo_count\n\ngpu_tpu_algo = gpu_tpu_algo.T\ngpu_tpu_algo = gpu_tpu_algo.rename(columns = \n{'Linear or Logistic Regression': 'Regression',\n 'Decision Trees or Random Forests': 'Decision Trees',\n 'Convolutional Neural Networks': 'CNN',\n 'Gradient Boosting Machines (xgboost, lightgbm, etc)': 'Gradient Boosting',\n 'Dense Neural Networks (MLPs, etc)': 'DNN',\n 'Recurrent Neural Networks': 'RNN',\n 'Bayesian Approaches': 'Bayesian',\n 'Transformer Networks (BERT, gpt-3, etc)': 'Transformer',\n 'Generative Adversarial Networks': 'GAN',\n 'Evolutionary Approaches': 'Evolutionary',\n 'None': 'None',\n 'Other': 'Other'})\n\ngpu_tpu_algo = gpu_tpu_algo.T\ngpu_tpu_algo = gpu_tpu_algo.sort_values(by = 'Google Cloud TPUs', ascending = False)\n\n# --------------\n# Plot the chart\n# --------------\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (22,8))\n\nplt.subplot(121)\nalgo_set.plot(kind = 'bar', ax = ax1)\nax1.set(title = \"ML Algo\")\nplt.axvline(x = 2.5, ymin= -0.05, ymax= 1.1, color='black', linestyle='dashed', linewidth=3)\nplt.annotate(\"Deep learning algos\", xytext=(2.8, 6000), xy=(8, 3500), \n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n\nfig.subplots_adjust(wspace=0.2)\n\nplt.subplot(122)\ngpu_tpu_algo.plot(kind = 'bar', ax = ax2)\nax2.set(title = \"ML algo and GPU/TPU in 2021\")\nplt.axvline(x = 2.5, ymin= -0.05, ymax= 1.1, color='black', linestyle='dashed', linewidth=3)\nplt.annotate(\"Less CNN user than Gradient boost, but higher demand in GPU/TPU\", xytext=(2.8, 3000), xy=(2, 2500), \n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.annotate(\"\", xytext=(3, 3000), xy=(2.8, 2400), \n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:08.367602Z","iopub.execute_input":"2021-11-23T05:01:08.368106Z","iopub.status.idle":"2021-11-23T05:01:10.063452Z","shell.execute_reply.started":"2021-11-23T05:01:08.368041Z","shell.execute_reply":"2021-11-23T05:01:10.062491Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test = get_professionals(data_2021, 'Q5')\n\n# ----------------------------------------------------------------------------------------\n# 2021 (#18 and #19), 2020 (#18 and #19), 2019 (#26 and #27) About Computer vision and NLP\n# ----------------------------------------------------------------------------------------\n\n# ---------------------------------\n# COMPUTER VISION YES OR NO in 2021\n# ---------------------------------\n\nvision = test[['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER']]\nnlp = test[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER']]\nvision = vision.fillna(0)\nvision[vision != 0] = 1\nvision_df = vision[vision==True].count(axis=0).rename_axis('Algo').reset_index(name='counts')\nvision_df = vision_df.set_index('Algo').T\nstuff_2020 = vision_df\n\nvision['yes'] = vision[['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_OTHER']].sum(axis = 1)\nvision['yes'] = vision['yes'].apply(lambda x: x>=1)\n#vision['yes'].value_counts()\nvision['no'] = (vision['yes'].apply(lambda x: x == 0) | vision['Q18_Part_6'].apply(lambda x: x == 1))\n#vision['no'].value_counts()\n\nvision['Q20'] = test['Q20']\nvision = vision.drop(columns=['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER'])\nvision.replace({False: 0, True: 1}, inplace=True)\nvision_df = vision[vision['yes'] == 1].groupby('Q20').size()\n\ntotal_boss = vision['Q20'].value_counts()\n\nboss = pd.DataFrame(vision_df)\ntotal_boss = pd.DataFrame(total_boss)\nfinal_boss = total_boss.join(boss)\nfinal_boss.rename(columns = {final_boss.columns[0]: 'NO', final_boss.columns[1]: 'YES'}, inplace = True)\n#final_boss\n\n# ---------------------\n# NLP YES OR NO in 2021\n# ---------------------\n\nnlp = test[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER']]\nnlp = nlp.fillna(0)\nnlp[nlp != 0] = 1\nnlp_df = nlp[nlp==True].count(axis=0).rename_axis('Algo').reset_index(name='counts')\nnlp_df = nlp_df.set_index('Algo').T\n\nnlp['yes'] = nlp[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_OTHER']].sum(axis = 1)\nnlp['yes'] = nlp['yes'].apply(lambda x: x>=1)\n#vision['yes'].value_counts()\nnlp['no'] = (nlp['yes'].apply(lambda x: x == 0) | nlp['Q19_Part_5'].apply(lambda x: x == 1))\n#vision['no'].value_counts()\n\nnlp['Q20'] = test['Q20']\nnlp = nlp.drop(columns=['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER'])\nnlp.replace({False: 0, True: 1}, inplace=True)\nnlp_df = nlp[nlp['yes'] == 1].groupby('Q20').size()\n\ntotal_boss2 = nlp['Q20'].value_counts()\n\nboss2 = pd.DataFrame(nlp_df)\ntotal_boss2 = pd.DataFrame(total_boss2)\nfinal_boss2 = total_boss.join(boss2)\nfinal_boss2.rename(columns = {final_boss2.columns[1]: 'hello'}, inplace = True)\n#final_boss2['perc'] = final_boss2['hello'] * 100 / final_boss2['Q20']\nfinal_boss2.rename(columns = {final_boss2.columns[0]: 'NO', final_boss2.columns[1]: 'YES'}, inplace = True)\nfinal_boss2\n\n# --------------- \n#  SUBPLOTS - 1x2\n# ---------------\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(18,11))\n\nplt.subplot(121)   #  subplot 1\nfinal_boss.plot(kind='barh', ax = ax1)\nax1.set(title = \"Computer Vision Yes / No\",\n       xlabel = \"Counts\",\n       ylabel = \"Industry\")\n\n\n\nfor i,j in zip(ax1.containers[0], ax1.containers[1]):\n\n    perc = j.get_width() / i.get_width()\n    perc = (perc*100).round(1)\n    non_perc = (100 - perc).round(1)\n    \n    width = i.get_width()\n    height = i.get_height()\n    x, y = i.get_xy()\n    ax1.annotate(f'{non_perc}%', (x + width, y + height*1.02), ha=\"left\", va=\"center\")\n    \n    width2 = j.get_width()\n    height2 = j.get_height()\n    x2, y2 = j.get_xy() \n    ax1.annotate(f'{perc}%', (x + width2, y2 + height2*1.02), ha=\"left\", va=\"center\")\n\nfig.subplots_adjust(wspace=1)\n\nplt.subplot(122)   #  subplot 2\nfinal_boss2.plot(kind='barh', ax = ax2)\nax2.set(title = \"NLP Yes / No\",\n       xlabel = \"Counts\",\n       ylabel = \"Industry\")\n\nfor i,j in zip(ax2.containers[0], ax2.containers[1]):\n\n    perc = j.get_width() / i.get_width()\n    perc = (perc*100).round(1)\n    non_perc = (100 - perc).round(2)\n    \n    width = i.get_width()\n    height = i.get_height()\n    x, y = i.get_xy()\n    ax2.annotate(f'{non_perc}%', (x + width, y + height*1.02), ha=\"left\", va=\"center\")\n    \n    width2 = j.get_width()\n    height2 = j.get_height()\n    x2, y2 = j.get_xy() \n    ax2.annotate(f'{perc}%', (x + width2, y2 + height2*1.02), ha=\"left\", va=\"center\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:10.065052Z","iopub.execute_input":"2021-11-23T05:01:10.065311Z","iopub.status.idle":"2021-11-23T05:01:11.677848Z","shell.execute_reply.started":"2021-11-23T05:01:10.065282Z","shell.execute_reply":"2021-11-23T05:01:11.677083Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"industry_gpu = ['Q20', 'Q12_Part_1']\nindustry_by_gpu = pros_2021[industry_gpu]\nindustry_by_gpu = industry_by_gpu[industry_by_gpu['Q20'].notna()]\n\nindustry_list = list(industry_by_gpu['Q20'].unique())\n\nind_gpu_merged = pd.DataFrame()\n\nfor idx, ind in enumerate(industry_list):\n    ind_gpu = industry_by_gpu.loc[industry_by_gpu['Q20'] == ind]\n    idx = pd.Series(ind_gpu['Q12_Part_1'].squeeze().values.ravel()).value_counts()\n    ind_gpu_merged[ind] = idx\n    \n    \nindustry_tpu = ['Q20', 'Q12_Part_2']\nindustry_by_tpu = pros_2021[industry_tpu]\nindustry_by_tpu = industry_by_tpu[industry_by_tpu['Q20'].notna()]\n\nind_tpu_merged = pd.DataFrame()\n\nfor idx, ind in enumerate(industry_list):\n    ind_tpu = industry_by_tpu.loc[industry_by_tpu['Q20'] == ind]\n    idx = pd.Series(ind_tpu['Q12_Part_2'].squeeze().values.ravel()).value_counts()\n    ind_tpu_merged[ind] = idx\n\nind_gpu_tpu = ind_gpu_merged.append(ind_tpu_merged).T\nind_gpu_tpu.columns = ind_gpu_tpu.columns.str.strip()\nind_gpu_tpu = ind_gpu_tpu.sort_values(by = 'NVIDIA GPUs', ascending = False)\n\nfig, ax = plt.subplots(figsize = (11,10))\nind_gpu_tpu.plot(kind = 'barh', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:11.678955Z","iopub.execute_input":"2021-11-23T05:01:11.679164Z","iopub.status.idle":"2021-11-23T05:01:12.155651Z","shell.execute_reply.started":"2021-11-23T05:01:11.679139Z","shell.execute_reply":"2021-11-23T05:01:12.154866Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Which of the following cloud computing platforms do you use on a regular basis? \n# 2021 Q27_A_Part_1\tQ27_A_Part_2\tQ27_A_Part_3\tQ27_A_Part_4\tQ27_A_Part_5\tQ27_A_Part_6\tQ27_A_Part_7\tQ27_A_Part_8\tQ27_A_Part_9\tQ27_A_Part_10\tQ27_A_Part_11\tQ27_A_OTHER\t\n# 2020 Q26_A_Part_1\tQ26_A_Part_2\tQ26_A_Part_3\tQ26_A_Part_4\tQ26_A_Part_5\tQ26_A_Part_6\tQ26_A_Part_7\tQ26_A_Part_8\tQ26_A_Part_9\tQ26_A_Part_10\tQ26_A_Part_11\tQ26_A_OTHER\n# 2019 Q29_Part_1\tQ29_Part_2\tQ29_Part_3\tQ29_Part_4\tQ29_Part_5\tQ29_Part_6\tQ29_Part_7\tQ29_Part_8\tQ29_Part_9\tQ29_Part_10\tQ29_Part_11\tQ29_Part_12\n\n# Do you use any of the following cloud computing products on a regular basis? (Select all that apply)\n# 2021  Q29_A_Part_1\tQ29_A_Part_2\tQ29_A_Part_3\tQ29_A_Part_4\tQ29_A_OTHER\n#Question 29-A (which specific AWS/Azure/GCP products) was only asked to respondents that selected the relevant answer choices for Question 27-A (which of the following companies).\n# 2020 Q27_A_Part_1\tQ27_A_Part_2\tQ27_A_Part_3\tQ27_A_Part_4\tQ27_A_Part_5\tQ27_A_Part_6\tQ27_A_Part_7\tQ27_A_Part_8\tQ27_A_Part_9\tQ27_A_Part_10\tQ27_A_Part_11\tQ27_A_OTHER\n# 2019 Q30_Part_1\tQ30_Part_2\tQ30_Part_3\tQ30_Part_4\tQ30_Part_5\tQ30_Part_6\tQ30_Part_7\tQ30_Part_8\tQ30_Part_9\tQ30_Part_10\tQ30_Part_11\n\n# Which of the following automated machine learning tools (or partial AutoML tools) do you use on aregular basis? (Select all that apply)\n# 2021 Q31_A_Part_1\tQ31_A_Part_2\tQ31_A_Part_3\tQ31_A_Part_4\tQ31_A_Part_5\tQ31_A_Part_6\tQ31_A_Part_7\tQ31_A_Part_8\tQ31_A_Part_9\tQ31_A_OTHER\n# 2020 Q28_A_Part_1\tQ28_A_Part_2\tQ28_A_Part_3\tQ28_A_Part_4\tQ28_A_Part_5\tQ28_A_Part_6\tQ28_A_Part_7\tQ28_A_Part_8\tQ28_A_Part_9\tQ28_A_Part_10\tQ28_A_OTHER\n# 2019 Q32_Part_1\tQ32_Part_2\tQ32_Part_3\tQ32_Part_4\tQ32_Part_5\tQ32_Part_6\tQ32_Part_7\tQ32_Part_8\tQ32_Part_9\tQ32_Part_10\tQ32_Part_11\tQ32_Part_12\tQ32_OTHER_TEXT\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:12.156686Z","iopub.execute_input":"2021-11-23T05:01:12.156888Z","iopub.status.idle":"2021-11-23T05:01:12.161372Z","shell.execute_reply.started":"2021-11-23T05:01:12.156863Z","shell.execute_reply":"2021-11-23T05:01:12.160801Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Commercial BI tool growth is higher than visualization library. Monetization of data analysis","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ml_manage_2021 = ['Q38_A_Part_1','Q38_A_Part_2','Q38_A_Part_3',\n                  'Q38_A_Part_4','Q38_A_Part_5','Q38_A_Part_6',\n                  'Q38_A_Part_7','Q38_A_Part_8','Q38_A_Part_9',\n                  'Q38_A_Part_10','Q38_A_Part_11','Q38_A_OTHER']\n\ndf_ml_manage_2021 = pros_2021[ml_manage_2021]\n#df_ml_manage_2021 = df_ml_manage_2021.rename(columns={'Q38_A_Part_1': 'Neptune.ai',\n#                                                      'Q38_A_Part_2': 'Weights & Biases',\n#                                                      'Q38_A_Part_3': 'Comet.ml',\n#                                                      'Q38_A_Part_4': 'Sacred + Omniboard',\n#                                                      'Q38_A_Part_5': 'TensorBoard',\n#                                                      'Q38_A_Part_6': 'Guild.ai',\n#                                                      'Q38_A_Part_7': 'Polyaxon',\n#                                                      'Q38_A_Part_8': 'Trains',\n#                                                      'Q38_A_Part_9': 'Domino Model Monitor',\n#                                                      'Q38_A_Part_10': 'MLflow',\n#                                                      'Q38_A_Part_11': 'None',\n#                                                      'Q38_A_OTHER': 'Other'})\n\n\ncount_ml_manage_2021 = pd.Series(df_ml_manage_2021[ml_manage_2021].squeeze().values.ravel()).value_counts()\ndf_count_ml_manage_2021 = pd.DataFrame(count_ml_manage_2021)\ndf_count_ml_manage_2021 = df_count_ml_manage_2021.reset_index()\ndf_count_ml_manage_2021.columns = ['manage', '2021']\n\n\n# ------------\n\nml_manage_2020 = ['Q35_A_Part_1','Q35_A_Part_2',\n          'Q35_A_Part_3','Q35_A_Part_4',\n          'Q35_A_Part_5','Q35_A_Part_6',\n          'Q35_A_Part_7','Q35_A_Part_8','Q35_A_Part_9','Q35_A_Part_10','Q35_A_OTHER']\n\n\ndf_ml_manage_2020 = pros_2020[ml_manage_2020]\n\n\ncount_ml_manage_2020 = pd.Series(df_ml_manage_2020[ml_manage_2020].squeeze().values.ravel()).value_counts()\ndf_count_ml_manage_2020 = pd.DataFrame(count_ml_manage_2020)\ndf_count_ml_manage_2020 = df_count_ml_manage_2020.reset_index()\ndf_count_ml_manage_2020.columns = ['manage', '2020']\n\n#df = pd.concat([df_count_ml_manage_2021, df_count_ml_manage_2020], axis = 1)\n#df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\nmerged = df_count_ml_manage_2021.set_index('manage').combine_first(df_count_ml_manage_2020.set_index('manage'))\nmerged = merged.sort_values(by=['2021'], ascending = False)\nmerged = merged.iloc[1:, :]\n\nfig, ax = plt.subplots(figsize=(16,8))\n\nmerged.plot(kind='bar', ax=ax)\n\nax.set(title = \"Tools used to manage machine learning experiments\")\n\nax.annotate('Newly added', xy=(1.2, 1200), xytext=(2, 1300),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n\nax.annotate('Newly added', xy=(5.2, 260), xytext=(5.6, 300),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n\nax.annotate('Removed', xy=(10.8, 260), xytext=(9.8, 350),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:12.162207Z","iopub.execute_input":"2021-11-23T05:01:12.162510Z","iopub.status.idle":"2021-11-23T05:01:12.624371Z","shell.execute_reply.started":"2021-11-23T05:01:12.162483Z","shell.execute_reply":"2021-11-23T05:01:12.623523Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"cloud_stocks = pd.read_excel('../input/cloud-apm-stocks/cloud_apm_stocks.xlsx')\n#cloud_stocks = cloud_stocks.rename(columns = {'Unnamed: 0': 'Revenue'})\ncloud_stocks = cloud_stocks.iloc[1:, :]\ncloud_stocks = cloud_stocks.set_index('Date')\ncloud_stocks = cloud_stocks.div(1000000000)\ncloud_stocks = cloud_stocks.rename(columns = \n                   {'DDOG.O (Fundamental)': 'DataDog',\n                    'NEWR.K (Fundamental)': 'New Relic',\n                    'DT (Fundamental)': 'Dynatrace',\n                    'ESTC.K (Fundamental)': 'Elastic NV',\n                    'CFLT.O (Fundamental)': 'CFLT'\n                   })\n\ncloud_stocks = cloud_stocks.drop(columns = ['CFLT'])\nfig, ax = plt.subplots(figsize=(18,6))\ncloud_stocks.plot(ax=ax)\n\nplt.xlabel('Date')\nplt.ylabel('Market cap (Billions)')\nplt.title('Application Performace Management(APM) companies market cap')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:12.625527Z","iopub.execute_input":"2021-11-23T05:01:12.625752Z","iopub.status.idle":"2021-11-23T05:01:12.984106Z","shell.execute_reply.started":"2021-11-23T05:01:12.625725Z","shell.execute_reply":"2021-11-23T05:01:12.983340Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"industry_gpu = ['Q20', 'Q12_Part_1']\nindustry_by_gpu = pros_2021[industry_gpu]\nindustry_by_gpu = industry_by_gpu[industry_by_gpu['Q20'].notna()]\n\nindustry_list = list(industry_by_gpu['Q20'].unique())\n\nind_gpu_merged = pd.DataFrame()\n\nfor idx, ind in enumerate(industry_list):\n    ind_gpu = industry_by_gpu.loc[industry_by_gpu['Q20'] == ind]\n    idx = pd.Series(ind_gpu['Q12_Part_1'].squeeze().values.ravel()).value_counts()\n    ind_gpu_merged[ind] = idx\n    \n    \nindustry_tpu = ['Q20', 'Q12_Part_2']\nindustry_by_tpu = pros_2021[industry_tpu]\nindustry_by_tpu = industry_by_tpu[industry_by_tpu['Q20'].notna()]\n\nind_tpu_merged = pd.DataFrame()\n\nfor idx, ind in enumerate(industry_list):\n    ind_tpu = industry_by_tpu.loc[industry_by_tpu['Q20'] == ind]\n    idx = pd.Series(ind_tpu['Q12_Part_2'].squeeze().values.ravel()).value_counts()\n    ind_tpu_merged[ind] = idx\n\nind_gpu_tpu = ind_gpu_merged.append(ind_tpu_merged).T\nind_gpu_tpu.columns = ind_gpu_tpu.columns.str.strip()\nind_gpu_tpu = ind_gpu_tpu.sort_values(by = 'NVIDIA GPUs', ascending = False)\n\nfig, ax = plt.subplots(figsize = (22,10))\nind_gpu_tpu.plot(kind = 'barh', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:12.985402Z","iopub.execute_input":"2021-11-23T05:01:12.985822Z","iopub.status.idle":"2021-11-23T05:01:13.478373Z","shell.execute_reply.started":"2021-11-23T05:01:12.985782Z","shell.execute_reply":"2021-11-23T05:01:13.477424Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"hardware_2021 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER']\nhardware_2021_df = pros_2021[hardware_2021]\ncount_hardware_2021 = pd.Series(hardware_2021_df[hardware_2021].squeeze().values.ravel()).value_counts()\n\nhardware_2020 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_OTHER']\nhardware_2020_df = pros_2020[hardware_2020]\ncount_hardware_2020 = pd.Series(hardware_2020_df[hardware_2020].squeeze().values.ravel()).value_counts()\n\nhardware_2019 = ['Q21_Part_1','Q21_Part_2','Q21_Part_3','Q21_Part_4','Q21_Part_5','Q21_OTHER_TEXT']\nhardware_2019_df = pros_2019[hardware_2019]\ncount_hardware_2019 = pd.Series(hardware_2019_df[hardware_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_hardware_2021 = pd.DataFrame(count_hardware_2021)\ndf_count_hardware_2021 = df_count_hardware_2021.reset_index()\ndf_count_hardware_2021.columns = ['manage', '2021']\n\ndf_count_hardware_2020 = pd.DataFrame(count_hardware_2020)\ndf_count_hardware_2020 = df_count_hardware_2020.reset_index()\ndf_count_hardware_2020.columns = ['manage', '2020']\n\ndf_count_hardware_2019 = pd.DataFrame(count_hardware_2019)\ndf_count_hardware_2019 = df_count_hardware_2019.reset_index()\ndf_count_hardware_2019.columns = ['manage', '2019']\n\ndf_count_hardware_2019 = df_count_hardware_2019.iloc[1:6]\ndf_count_hardware_2019 = df_count_hardware_2019.set_index('manage').T[['GPUs', 'TPUs','Other']]\n\ndf_count_hardware_2020 = df_count_hardware_2020.set_index('manage').T[['GPUs', 'TPUs', 'Other']]\n\ndf_count_hardware_2021 = df_count_hardware_2021.set_index('manage').T\ndf_count_hardware_2021.columns = df_count_hardware_2021.columns.str.strip()\ndf_count_hardware_2021 = df_count_hardware_2021[['NVIDIA GPUs','Google Cloud TPUs','Other','AWS Inferentia Chips','AWS Trainium Chips']]\ndf_count_hardware_2021 = df_count_hardware_2021.rename(columns = {'NVIDIA GPUs': 'GPUs',\n                                                                    'Google Cloud TPUs': 'TPUs',\n                                                                    'Other': 'Other',\n                                                                    'AWS Inferentia Chips': 'AWS Inferentia',\n                                                                    'AWS Trainium Chips': 'AWS Trainium'})\n\ndf_hardware_merged = df_count_hardware_2019.append(df_count_hardware_2020).append(df_count_hardware_2021).fillna(0)\n\nfig, ax = plt.subplots(figsize=(18,6))\ndf_hardware_merged.plot(kind='bar', ax=ax)\nax.set(title = 'Special hardware usage trend',\n    xlabel = 'year',\n      ylabel = 'number of respondants',\n      )\n\nax.annotate(\"AWS's custom chips are added in 2021\", xy=(2.1, 1000), xytext=(2.3, 2000),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:13.479771Z","iopub.execute_input":"2021-11-23T05:01:13.480295Z","iopub.status.idle":"2021-11-23T05:01:13.807605Z","shell.execute_reply.started":"2021-11-23T05:01:13.480249Z","shell.execute_reply":"2021-11-23T05:01:13.806245Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_storage_regular_2021 = ['Q30_A_Part_1', 'Q30_A_Part_2','Q30_A_Part_3', 'Q30_A_Part_4', 'Q30_A_Part_5', 'Q30_A_Part_6', 'Q30_A_Part_7', 'Q30_A_OTHER']\n\ndf_data_storage_regular_2021 = pros_2021[data_storage_regular_2021]\ncount_data_storage_regular_2021 = pd.Series(df_data_storage_regular_2021[data_storage_regular_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_data_storage_regular_2021 = pd.DataFrame(count_data_storage_regular_2021)\ndf_count_data_storage_regular_2021 = df_count_data_storage_regular_2021.reset_index()\ndf_count_data_storage_regular_2021.columns = ['Storage', 'Counts']\ndf_count_data_storage_regular_2021 = df_count_data_storage_regular_2021.set_index('Storage').T\n\n\n\nfig, ax1 = plt.subplots(figsize = (11,8))\n\ndf_count_data_storage_regular_2021.plot(kind='bar', ax = ax1)\nax1.set(title=\"Bonus, respondant who uses cloud data storage product on a regular basis\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:01:13.809056Z","iopub.execute_input":"2021-11-23T05:01:13.809391Z","iopub.status.idle":"2021-11-23T05:01:14.066921Z","shell.execute_reply.started":"2021-11-23T05:01:13.809350Z","shell.execute_reply":"2021-11-23T05:01:14.064909Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}